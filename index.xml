<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GMG</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>GMG</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Grainne McGuire 2020 [CC BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/)</copyright><lastBuildDate>Thu, 07 Nov 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>GMG</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Traditional-style reserving using GLMs</title>
      <link>/post/traditional-style-reserving-using-glms/</link>
      <pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/post/traditional-style-reserving-using-glms/</guid>
      <description>


&lt;p&gt;This post provides a worked example in R of fitting a GLM to some non-life claims reserving data.
The example and data are drawn from the CAS Monograph &lt;a href=&#34;/publication/2016_monograph&#34;&gt;Stochastic Loss Reserving using Generalized Linear Models&lt;/a&gt;.
Here we follow through the application of the cross-classified model from that monograph to the data (Chapter 3), and follow through with the additonal work to firstly simplify the model and secondly to improve the model fit through the use of some interaction terms (Chapter 7).&lt;/p&gt;
&lt;p&gt;If you want to access the underlying Rnotebook directly, you may access it at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/grainnemcguire/code_snippets/blob/master/R/2019-11-07-traditional-style-reserving-using-glms.Rmd&#34;&gt;2019-11-07-traditional-style-reserving-using-glms.Rmd&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The data used is available in this &lt;a href=&#34;https://github.com/grainnemcguire/code_snippets/blob/master/data/glms_meyershi.csv&#34;&gt;csv file&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;Before we begin, we first set up the R session by loading in the various packages that we need.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cran.rstudio.com/web/packages/here/index.html&#34;&gt;here&lt;/a&gt;: I first heard about here from reading &lt;a href=&#34;https://github.com/jennybc/here_here&#34;&gt;Jenny Bryan’s&lt;/a&gt; article on it and have been a fan of &lt;code&gt;here&lt;/code&gt; and the R project structure ever since. It really helps with portability of code.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basically it allows you to use relative rather than absolute file paths.&lt;/li&gt;
&lt;li&gt;If you want to run this notebook and don’t want to use &lt;code&gt;here&lt;/code&gt; then all you need to do is put an appropriate pathname in for loading in the data from a CSV file.
Location is not used anywhere else.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/Rdatatable/data.table&#34;&gt;data.table&lt;/a&gt; I really like &lt;code&gt;data.table&lt;/code&gt; - its power, speed and terseness. At some point though I may replace the &lt;code&gt;data.table&lt;/code&gt; code with base R to reduce dependencies. For now though, there isn’t a huge amount of &lt;code&gt;data.table&lt;/code&gt; code.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Even if you don’t like &lt;code&gt;data.table&lt;/code&gt; syntax, the &lt;code&gt;fread&lt;/code&gt; and &lt;code&gt;fwrite&lt;/code&gt; functions can be very useful for reading and writing CSV files.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ggplot2.tidyverse.org/&#34;&gt;ggplot2&lt;/a&gt;: create nice graphs easily&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html&#34;&gt;viridis&lt;/a&gt; nice colour palettes that are tested for common forms of colour-blindness&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/cowplot/index.html&#34;&gt;cowplot&lt;/a&gt; - an easy way of grouping graphs into a single figure&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html&#34;&gt;magrittr&lt;/a&gt; Use the &lt;code&gt;%&amp;gt;%&lt;/code&gt; pipe to reduce nested parentheses&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/knitr/index.html&#34;&gt;knitr&lt;/a&gt; The notebook uses &lt;code&gt;kable&lt;/code&gt; from the &lt;code&gt;knitr&lt;/code&gt; package. If you’re using RStudio to run this code in notebook format, then you should already have it. Otherwise you can install it, or you can simply replace all the &lt;code&gt;kable&lt;/code&gt; commands with print statements.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you don’t have any of these packages you will need to install them via &lt;code&gt;install.packages&lt;/code&gt; or, if using RStudio, via the Install buttom in the packages pane.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
library(here)
library(data.table)
library(ggplot2)
library(viridis)
library(cowplot)
library(magrittr)
# I have elected not to attach knitr, so we need to use knitr::kable() below

options(scipen = 99)   # get rid of scientific notation
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;The data are presented in Section 1.3 of the monograph. Ultimately the data were sourced from the Meyers and Shi (2011) database, and are the workers compensation triangle of the New Jersey Manufacturers Group.&lt;/p&gt;
&lt;p&gt;For ease of use, I have created a CSV file with the data which is loaded in this code chunk. As noted above I use relative path names with the &lt;code&gt;here&lt;/code&gt; package. If you don’t want to have a setup that works with &lt;code&gt;here&lt;/code&gt;, just ensure the full pathname to the file is included in the &lt;code&gt;fread&lt;/code&gt; statement below.&lt;/p&gt;
&lt;p&gt;Once the data is loaded in, have a look at the start of it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;msdata &amp;lt;- data.table::fread(here::here(&amp;quot;data/2019-11-07-glms_meyershi.csv&amp;quot;))
# if needed replace here::here(&amp;quot;data/glms_meyershi.csv&amp;quot;) with
# the correct path and filename of where you put the data

setDT(msdata)

print(msdata[1:6,])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    acc_year dev_year cumulative incremental
## 1:        1        1      41821       41821
## 2:        1        2      76550       34729
## 3:        1        3      96697       20147
## 4:        1        4     112662       15965
## 5:        1        5     123947       11285
## 6:        1        6     129871        5924&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we have four columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;acc_year: accident year, numbered from 1 to 10&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;dev_year: development year, also numbered from 1 to 10&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;cumulative: cumulative payments to date&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;incremental: incremental payments for that accident year, development year combination.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s look at the data visually.&lt;/p&gt;
&lt;p&gt;First we plot the cumulative amounts in each accident year&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data=msdata, aes(x=dev_year, y=cumulative, colour=as.factor(acc_year))) +
    geom_line(size=1) +
    scale_color_viridis_d(begin=0.9, end=0) + 
    ggtitle(&amp;quot;Cumulative amounts by development year&amp;quot;) + 
    theme_bw() + 
    theme(legend.position = &amp;quot;right&amp;quot;, legend.title=element_blank(), legend.text=element_text(size=8))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-07-traditional-style-reserving-using-glms_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now look at the incremental amounts&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data=msdata, aes(x=dev_year, y=incremental, colour=as.factor(acc_year))) +
    geom_line(size=1) +
    scale_color_viridis_d(begin=0.9, end=0) + 
    ggtitle(&amp;quot;Incremental amounts by development year&amp;quot;) + 
    theme_bw() + 
    theme(legend.position = &amp;quot;right&amp;quot;, legend.title=element_blank(), legend.text=element_text(size=8))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-07-traditional-style-reserving-using-glms_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The data look quite well behaved - each year seems to have a similar development pattern.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modelling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modelling&lt;/h2&gt;
&lt;div id=&#34;initial-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Initial model&lt;/h3&gt;
&lt;p&gt;The first model applied here is the Over-dispersed Poisson (ODP) cross classified (cc) model (Sections 3.3.2 and 3.3.3 of the monograph).
This model has been shown to give the same results as the chain ladder algorithm.&lt;/p&gt;
&lt;p&gt;To apply the model, we will use the &lt;code&gt;glm&lt;/code&gt; function from the base R &lt;code&gt;stats&lt;/code&gt; package. The cross-classified model requires separate levels for each of accident and development year so we first make a factor version of these variates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;msdata[, acc_year_factor := as.factor(acc_year)
       ][, dev_year_factor := as.factor(dev_year)
         ][, cal_year := acc_year + dev_year - 1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we fit the model and look at the results via &lt;code&gt;summary&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The family is the &lt;em&gt;quasipoisson&lt;/em&gt; - this is how we fit an ODP model with &lt;code&gt;glm&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The link is log&lt;/li&gt;
&lt;li&gt;The formula is simply “incremental ~ 0 + acc_year_factor + dev_year_factor”
&lt;ul&gt;
&lt;li&gt;The 0 tells &lt;code&gt;glm&lt;/code&gt; to fit a model without an intercept&lt;/li&gt;
&lt;li&gt;We choose to do that here because then we can more easily compare the results to those in the monograph.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm_fit1 &amp;lt;- glm(data = msdata, 
    family = quasipoisson(link = &amp;quot;log&amp;quot;),
    formula = &amp;quot;incremental ~ 0 + acc_year_factor + dev_year_factor&amp;quot;)


summary(glm_fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = &amp;quot;incremental ~ 0 + acc_year_factor + dev_year_factor&amp;quot;, 
##     family = quasipoisson(link = &amp;quot;log&amp;quot;), data = msdata)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -21.493   -5.534    0.000    5.136   21.059  
## 
## Coefficients:
##                   Estimate Std. Error t value             Pr(&amp;gt;|t|)    
## acc_year_factor1  10.65676    0.03164 336.794 &amp;lt; 0.0000000000000002 ***
## acc_year_factor2  10.79533    0.02994 360.507 &amp;lt; 0.0000000000000002 ***
## acc_year_factor3  10.89919    0.02887 377.465 &amp;lt; 0.0000000000000002 ***
## acc_year_factor4  10.98904    0.02808 391.326 &amp;lt; 0.0000000000000002 ***
## acc_year_factor5  11.03883    0.02783 396.654 &amp;lt; 0.0000000000000002 ***
## acc_year_factor6  11.01590    0.02855 385.867 &amp;lt; 0.0000000000000002 ***
## acc_year_factor7  11.00808    0.02945 373.734 &amp;lt; 0.0000000000000002 ***
## acc_year_factor8  10.89050    0.03266 333.463 &amp;lt; 0.0000000000000002 ***
## acc_year_factor9  10.83613    0.03669 295.348 &amp;lt; 0.0000000000000002 ***
## acc_year_factor10 10.69108    0.05104 209.454 &amp;lt; 0.0000000000000002 ***
## dev_year_factor2  -0.20466    0.02276  -8.993  0.00000000009767316 ***
## dev_year_factor3  -0.74741    0.02819 -26.512 &amp;lt; 0.0000000000000002 ***
## dev_year_factor4  -1.01667    0.03284 -30.954 &amp;lt; 0.0000000000000002 ***
## dev_year_factor5  -1.45160    0.04214 -34.446 &amp;lt; 0.0000000000000002 ***
## dev_year_factor6  -1.83254    0.05471 -33.495 &amp;lt; 0.0000000000000002 ***
## dev_year_factor7  -2.14026    0.07150 -29.933 &amp;lt; 0.0000000000000002 ***
## dev_year_factor8  -2.34827    0.09312 -25.218 &amp;lt; 0.0000000000000002 ***
## dev_year_factor9  -2.51317    0.12673 -19.831 &amp;lt; 0.0000000000000002 ***
## dev_year_factor10 -2.66449    0.19930 -13.369  0.00000000000000158 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 114.5364)
## 
##     Null deviance: 27479374.2  on 55  degrees of freedom
## Residual deviance:     4128.1  on 36  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now extract the coefficient table in a more convenient way and append it onto the &lt;code&gt;glm_fit1&lt;/code&gt; object for later use.&lt;/p&gt;
&lt;p&gt;We will also print the table again in a nicer form to make it easier to compare to the first 3 columns of Table 3-5 of the monograph.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you do this, you should see that the results match.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# save the data for later use
glm_fit1$coeff_table &amp;lt;- data.table(parameter = names(glm_fit1$coefficients), 
                                   coeff_glm_fit1 = glm_fit1$coefficients)

# print out the table so we can compare against Table 3-5.
glm_fit1$coeff_table %&amp;gt;% 
    knitr::kable(digits=c(0,3))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;parameter&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;coeff_glm_fit1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;acc_year_factor1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.657&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;acc_year_factor2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.795&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;acc_year_factor3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.899&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;acc_year_factor4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.989&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;acc_year_factor5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.039&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;acc_year_factor6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;acc_year_factor7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.008&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;acc_year_factor8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.891&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;acc_year_factor9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.836&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;acc_year_factor10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.691&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dev_year_factor2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.205&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dev_year_factor3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.747&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dev_year_factor4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dev_year_factor5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.452&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dev_year_factor6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.833&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dev_year_factor7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.140&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dev_year_factor8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.348&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dev_year_factor9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.513&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dev_year_factor10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.664&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;loss-reserve&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loss reserve&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We will calculate the loss reserve for this model&lt;/li&gt;
&lt;li&gt;This should give the same answers as the chain ladder algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# first make the lower triangle data set
ay &amp;lt;- NULL
dy &amp;lt;- NULL


for(i in 2:10){
    ay &amp;lt;- c(ay, rep(i, times=(i-1)))
    dy &amp;lt;- c(dy, (10-i+2):10)
}

futdata &amp;lt;- data.table(acc_year = ay, dev_year = dy)

# make factors
futdata[, cal_year := acc_year + dev_year
        ][, acc_year_factor := as.factor(acc_year)
          ][, dev_year_factor := as.factor(dev_year)]

# make the prediction and sum by acc_year
x &amp;lt;- predict(glm_fit1, newdata = futdata, type=&amp;quot;response&amp;quot;)
futdata[, incremental := x]


ocl_year &amp;lt;- futdata[,  lapply(.SD, sum), .SDcols=c(&amp;quot;incremental&amp;quot;), by=&amp;quot;acc_year&amp;quot;]

ocl_year %&amp;gt;% 
    knitr::kable(digits=c(0, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;acc_year&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;incremental&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3398&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8155&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14579&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22645&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;31865&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;45753&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;60093&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;80983&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;105874&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;As expected, this matches the results in Table 3-2 of the monograph.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;model-diagnostics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model diagnostics&lt;/h3&gt;
&lt;p&gt;It’s always important to check that a model fits the data well, so here we look at the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Residual Scatterplots
&lt;ul&gt;
&lt;li&gt;by linear predictor&lt;/li&gt;
&lt;li&gt;by accident, development and calendar years&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Heat map of actual vs fitted
&lt;ul&gt;
&lt;li&gt;In this we get the actual/fitted ratio in each (acc, dev) cell [subject to lower and upper bounds of (0.5, 2)] and then plot the colour-coded triangle of the actual/fitted values&lt;/li&gt;
&lt;li&gt;heat maps are helpful to check for model fit and may help to identify missing interactions.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note on residuals with &lt;code&gt;glm&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The residuals in a glm object accessed with &lt;code&gt;$residuals&lt;/code&gt; are residuals used in the model fitting algorithm.&lt;/li&gt;
&lt;li&gt;For diagnostic purposes, the standardised deviance residuals are usually preferable to work with.
&lt;ul&gt;
&lt;li&gt;These are the signed square roots of the contribution of the i&lt;em&gt;th&lt;/em&gt; observation to the deviance, divided by hat matrix values.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;stats::rstandard()&lt;/code&gt; function may be used with glm objects to extract the standardised deviance residuals.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Generating the diagnostics&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First we prepare the data by adding the fitted values and residuals.
&lt;ul&gt;
&lt;li&gt;Because this model has a lot of parameters, there are two observations where the fitted is exactly equal to the actual – (acc_year=1, dev_year=10) and (acc_year=10, dev_year=0).
This is because these observations have a unique parameter.&lt;/li&gt;
&lt;li&gt;The deviance calculations below return NaN (not a number) for these points, but the residual should really be 0 so this adjustment is made manually.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Also add actual/fitted ratios and the log of these (restricted to the range [log(0.5), log(2)]) - these will be used for a heatmap later.
&lt;ul&gt;
&lt;li&gt;The restricted range is used to generate easier to read shadings in the heat-map, while the conversion to log means that the shading scales will be similar intensity for &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;% and &lt;span class=&#34;math inline&#34;&gt;\(1/x\)&lt;/span&gt; %&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;msdata[, residuals1 := rstandard(glm_fit1)
       ][, fitted1 := glm_fit1$fitted.values
         ][, linear_predictor1 := log(fitted1)
           ][, AvsF1 := incremental / fitted1
             ][, AvsF_restricted1 := log(pmax(0.5, pmin(2,AvsF1)))]

# check for NaN residuals
msdata[is.nan(residuals1),]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    acc_year dev_year cumulative incremental acc_year_factor dev_year_factor
## 1:        1       10     144781        2958               1              10
## 2:       10        1      43962       43962              10               1
##    cal_year residuals1 fitted1 linear_predictor1 AvsF1
## 1:       10        NaN    2958          7.992269     1
## 2:       10        NaN   43962         10.691081     1
##             AvsF_restricted1
## 1: -0.0000000000000019984014
## 2:  0.0000000000000008881784&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# these occur where we expect them so so replace with 0
msdata[is.nan(residuals1), residuals1 := 0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Look at the first 10 rows of msdata&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(msdata, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     acc_year dev_year cumulative incremental acc_year_factor dev_year_factor
##  1:        1        1      41821       41821               1               1
##  2:        1        2      76550       34729               1               2
##  3:        1        3      96697       20147               1               3
##  4:        1        4     112662       15965               1               4
##  5:        1        5     123947       11285               1               5
##  6:        1        6     129871        5924               1               6
##  7:        1        7     134646        4775               1               7
##  8:        1        8     138388        3742               1               8
##  9:        1        9     141823        3435               1               9
## 10:        1       10     144781        2958               1              10
##     cal_year  residuals1   fitted1 linear_predictor1     AvsF1
##  1:        1 -0.37704981 42478.725         10.656759 0.9845164
##  2:        2  0.06821815 34616.808         10.452095 1.0032410
##  3:        3  0.02211088 20117.514          9.909346 1.0014657
##  4:        4  0.50192703 15368.757          9.640092 1.0387958
##  5:        5  1.36344235  9948.355          9.205163 1.1343584
##  6:        6 -1.13119533  6796.876          8.824218 0.8715769
##  7:        7 -0.33754581  4996.553          8.516503 0.9556589
##  8:        8 -0.56680264  4058.159          8.308485 0.9220929
##  9:        9 -0.01379476  3441.253          8.143591 0.9981829
## 10:       10  0.00000000  2958.000          7.992269 1.0000000
##             AvsF_restricted1
##  1: -0.015604749951508145936
##  2:  0.003235741327323749146
##  3:  0.001464610789021573859
##  4:  0.038062125103388820546
##  5:  0.126067171138098593763
##  6: -0.137451186377785167236
##  7: -0.045354245283910396558
##  8: -0.081109303248732236846
##  9: -0.001818723883641001036
## 10: -0.000000000000001998401&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s look at the residual scatterplots - here I use the &lt;code&gt;cowplot&lt;/code&gt; package to combine all 4 graphs into one plot.&lt;/p&gt;
&lt;p&gt;In the linear predictor scatterplot, the points are colour coded so that the lighter points belong to the earlier development years, and the darker points belong to the later ones.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- ggplot(data=msdata, aes(x=linear_predictor1, y=residuals1, colour=dev_year)) +
    geom_point(size=2) +
    scale_colour_viridis(begin=0.9, end=0) +
    theme_bw() + 
    theme(legend.position = &amp;quot;none&amp;quot;) +
    ggtitle(&amp;quot;Linear predictor&amp;quot;)


p2 &amp;lt;- ggplot(data=msdata, aes(x=acc_year, y=residuals1)) +
    geom_point(size=2, colour=&amp;quot;#2d708eff&amp;quot;) +
    theme_bw() + 
    ggtitle(&amp;quot;Accident year&amp;quot;)

p3 &amp;lt;- ggplot(data=msdata, aes(x=dev_year, y=residuals1)) +
    geom_point(size=2, colour=&amp;quot;#2d708eff&amp;quot;) +
    theme_bw() + 
    ggtitle(&amp;quot;Development year&amp;quot;)

p4 &amp;lt;- ggplot(data=msdata, aes(x=cal_year, y=residuals1)) +
    geom_point(size=2, colour=&amp;quot;#2d708eff&amp;quot;) +
    theme_bw() + 
    ggtitle(&amp;quot;Calendar year&amp;quot;)

p &amp;lt;- plot_grid(p1, p2, p3, p4, nrow=2, rel_widths = c(1,1,1,1))

p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-07-traditional-style-reserving-using-glms_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now construct and draw the heat map. Note that the colours are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;blue (A/F = 50%)&lt;/li&gt;
&lt;li&gt;white (A/F = 100%)&lt;/li&gt;
&lt;li&gt;red (A/F = 200%)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# heatmap code
# to get the correct shading I&amp;#39;ve plotted the log of the restricted A/F values

p_hm &amp;lt;- ggplot(data=msdata, aes(x=dev_year, y=acc_year)) + 
    geom_tile(aes(fill = AvsF_restricted1))+scale_y_reverse()+
    scale_fill_gradient2(name=&amp;quot;AvF_min&amp;quot;, low=&amp;quot;royalblue&amp;quot;, mid=&amp;quot;white&amp;quot;, high=&amp;quot;red&amp;quot;, midpoint=0, space=&amp;quot;Lab&amp;quot;, na.value=&amp;quot;grey50&amp;quot;, guide=&amp;quot;colourbar&amp;quot;)+
    labs(x=&amp;quot;Development year&amp;quot;, y=&amp;quot;Accident year&amp;quot;)+
    theme(legend.position = &amp;quot;none&amp;quot;)+
    theme(axis.title.x = element_text(size=8), axis.text.x  = element_text(size=7))+
    theme(axis.title.y = element_text(size=8), axis.text.y  = element_text(size=7))+
    theme(panel.background = element_rect(fill = &amp;quot;grey&amp;quot;, colour = &amp;quot;grey&amp;quot;, size = 2, linetype = &amp;quot;solid&amp;quot;),
          panel.grid = element_line(colour=&amp;quot;grey&amp;quot;)) + 
    NULL

print(p_hm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-07-traditional-style-reserving-using-glms_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;refining-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Refining the model&lt;/h2&gt;
&lt;p&gt;We could stop here - and just use the results from this model, which match those produced by the chain ladder. The diagnostics suggest that the model fits quite well.
However can we:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;identify simplifications to the model to make it more parsinomious?&lt;/li&gt;
&lt;li&gt;identify any areas of poorer fit than may suggest missing model terms including interactions?&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;simplifying-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Simplifying the model&lt;/h3&gt;
&lt;p&gt;First we consider if we can use a parametric shape for the accident and development year parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;accident-year&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Accident year&lt;/h3&gt;
&lt;p&gt;First plot the accident year parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the data
dt_acc_year &amp;lt;- glm_fit1$coeff_table[grepl(&amp;quot;acc_year&amp;quot;, parameter),  
                                    ][, acc_year := as.integer(gsub(&amp;quot;acc_year_factor&amp;quot;, &amp;quot;&amp;quot;, parameter))]


# plot
ggplot(data=dt_acc_year, aes(x=acc_year, y=coeff_glm_fit1)) +
    geom_line(size=2, colour=&amp;quot;#440154ff&amp;quot;) +
    geom_point(size=4, colour=&amp;quot;#440154ff&amp;quot;) + 
    theme_bw() + 
    ggtitle(&amp;quot;Accident year parameters&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-07-traditional-style-reserving-using-glms_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Note that their shape closely resembles that of a parabola.&lt;/li&gt;
&lt;li&gt;This suggests that we can replace the 10 accident year parameters by
&lt;ul&gt;
&lt;li&gt;the overall intercept&lt;/li&gt;
&lt;li&gt;an acc_year term&lt;/li&gt;
&lt;li&gt;an acc_year squarted term&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;So refit the model on this basis.
&lt;ul&gt;
&lt;li&gt;Drop the 0 from the glm_fit1 formula to allow the model to have an intecept&lt;/li&gt;
&lt;li&gt;Replace the acc_year_factor term with the parabola terms.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# add an x and x^2 term
msdata[, acc_year_2 := acc_year^2]

glm_fit2 &amp;lt;- glm(data = msdata, 
    family = quasipoisson(link = &amp;quot;log&amp;quot;),
    formula = &amp;quot;incremental ~ acc_year + acc_year_2 + dev_year_factor&amp;quot;)


summary(glm_fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = &amp;quot;incremental ~ acc_year + acc_year_2 + dev_year_factor&amp;quot;, 
##     family = quasipoisson(link = &amp;quot;log&amp;quot;), data = msdata)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -28.5517   -5.1747    0.2691    4.5827   24.5421  
## 
## Coefficients:
##                    Estimate Std. Error t value             Pr(&amp;gt;|t|)    
## (Intercept)       10.470978   0.034414 304.264 &amp;lt; 0.0000000000000002 ***
## acc_year           0.200075   0.014219  14.071 &amp;lt; 0.0000000000000002 ***
## acc_year_2        -0.017907   0.001356 -13.210 &amp;lt; 0.0000000000000002 ***
## dev_year_factor2  -0.205555   0.021276  -9.661     0.00000000000243 ***
## dev_year_factor3  -0.750108   0.026492 -28.314 &amp;lt; 0.0000000000000002 ***
## dev_year_factor4  -1.014806   0.030982 -32.755 &amp;lt; 0.0000000000000002 ***
## dev_year_factor5  -1.451958   0.039797 -36.484 &amp;lt; 0.0000000000000002 ***
## dev_year_factor6  -1.830488   0.051662 -35.432 &amp;lt; 0.0000000000000002 ***
## dev_year_factor7  -2.142154   0.067504 -31.734 &amp;lt; 0.0000000000000002 ***
## dev_year_factor8  -2.352674   0.087924 -26.758 &amp;lt; 0.0000000000000002 ***
## dev_year_factor9  -2.513722   0.119637 -21.011 &amp;lt; 0.0000000000000002 ***
## dev_year_factor10 -2.660878   0.187820 -14.167 &amp;lt; 0.0000000000000002 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 102.5776)
## 
##     Null deviance: 750824  on 54  degrees of freedom
## Residual deviance:   4427  on 43  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see in the coefficient table part of the summary that the two acc_year terms are highly significant.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Now extract the coefficients and compare the previous and current fits.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Remember that the intercept must be included in these calculations.&lt;/li&gt;
&lt;li&gt;Again, save the coefficient table in this extracted form with the glm_fit2 object for later use.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the coefficient table
glm_fit2$coeff_table &amp;lt;- data.table(parameter = names(glm_fit2$coefficients), coeff_glm_fit2 = glm_fit2$coefficients)
print(glm_fit2$coeff_table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             parameter coeff_glm_fit2
##  1:       (Intercept)    10.47097818
##  2:          acc_year     0.20007497
##  3:        acc_year_2    -0.01790686
##  4:  dev_year_factor2    -0.20555514
##  5:  dev_year_factor3    -0.75010823
##  6:  dev_year_factor4    -1.01480620
##  7:  dev_year_factor5    -1.45195754
##  8:  dev_year_factor6    -1.83048769
##  9:  dev_year_factor7    -2.14215388
## 10:  dev_year_factor8    -2.35267361
## 11:  dev_year_factor9    -2.51372160
## 12: dev_year_factor10    -2.66087765&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now compare the past and current parameter estimates for accident year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# pull out the acc year coefficinents only
dt_acc_year[, coeff_glm_fit2 := glm_fit2$coeff_table[parameter == &amp;quot;acc_year&amp;quot;, coeff_glm_fit2]*acc_year + 
                glm_fit2$coeff_table[parameter == &amp;quot;acc_year_2&amp;quot;, coeff_glm_fit2]*acc_year^2 + 
                glm_fit2$coeff_table[parameter == &amp;quot;(Intercept)&amp;quot;, coeff_glm_fit2]]

# make long for ggplot
dt_acc_year_plot &amp;lt;- melt(dt_acc_year, id.vars = &amp;quot;acc_year&amp;quot;, measure.vars = c(&amp;quot;coeff_glm_fit1&amp;quot;, &amp;quot;coeff_glm_fit2&amp;quot;), variable.name=&amp;quot;model&amp;quot;, value = &amp;quot;estimate&amp;quot;)

# remove the coeff_ from the model names
dt_acc_year_plot[, model := gsub(&amp;quot;coeff_&amp;quot;, &amp;quot;&amp;quot;, model, fixed=TRUE)]

ggplot(data=dt_acc_year_plot, aes(x=acc_year, y=estimate, colour=model)) +
    geom_line(size=2) +
    geom_point(size=4) +
    scale_colour_viridis_d(begin=0, end=0.5) + 
    theme_bw() + 
    ggtitle(&amp;quot;Accident year parameters&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-07-traditional-style-reserving-using-glms_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This looks very good - the fit is very similar, but we have 7 fewer parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;development-year&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Development year&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Now we do the same thing for development year&lt;/li&gt;
&lt;li&gt;Note that the glm_fit2 model (and the glm_fit1 model too) do not have a parameter for dev_year = 1 as this is the base level.
&lt;ul&gt;
&lt;li&gt;This means that the parameter is really 0, so we must remember to include this.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract the data
dt_dev_year &amp;lt;- glm_fit2$coeff_table[grepl(&amp;quot;dev_year&amp;quot;, parameter),  
                                    ][, dev_year := as.integer(gsub(&amp;quot;dev_year_factor&amp;quot;, &amp;quot;&amp;quot;, parameter))][]   # known data.table printing bug

# add year 1
dt_dev_year &amp;lt;- rbind(dt_dev_year, data.table(parameter=&amp;quot;dev_year_factor1&amp;quot;, coeff_glm_fit2=0, dev_year=1))
setorder(dt_dev_year, dev_year)


# plot
ggplot(data=dt_dev_year, aes(x=dev_year, y=coeff_glm_fit2)) +
    geom_line(size=2, colour=&amp;quot;#440154ff&amp;quot;) +
    geom_point(size=4, colour=&amp;quot;#440154ff&amp;quot;) +
    theme_bw() +
    ggtitle(&amp;quot;Development year parameters&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-07-traditional-style-reserving-using-glms_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Looking at this plot, it appears that a straight line would fit quite well&lt;/li&gt;
&lt;li&gt;This fit would be improved by allowing the straight line to bend (have a knot) at dev_year = 7
&lt;ul&gt;
&lt;li&gt;So let’s try this below&lt;/li&gt;
&lt;li&gt;note we actually fit dev_year - 1 rather than dev_year
&lt;ul&gt;
&lt;li&gt;this means that the parameter estimate at dev_year = 1 is 0, just as it is in the glm_fit2 model, so it makes the results comparable&lt;/li&gt;
&lt;li&gt;if we fit dev_year, then the parameter estimate at dev_year=1 would be non-zero, so the two fits would be shifted relative to each other and we would need to adjust for that.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# add dev-1 and dev-7 terms
msdata[, dev_year_m1 := dev_year - 1]
msdata[, dev_year_ge_7 := pmax(dev_year-7.5, 0)]

# fit the model
glm_fit3 &amp;lt;- glm(data = msdata, 
    family = quasipoisson(link = &amp;quot;log&amp;quot;),
    formula = &amp;quot;incremental ~ acc_year + acc_year_2 + dev_year_m1 + dev_year_ge_7&amp;quot;)

# extract and save the coefficient table
glm_fit3$coeff_table &amp;lt;- data.table(parameter = names(glm_fit3$coefficients), coeff_glm_fit3 = glm_fit3$coefficients)

# display a summary of the model
summary(glm_fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = &amp;quot;incremental ~ acc_year + acc_year_2 + dev_year_m1 + dev_year_ge_7&amp;quot;, 
##     family = quasipoisson(link = &amp;quot;log&amp;quot;), data = msdata)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -25.301   -9.262   -2.080    5.893   42.841  
## 
## Coefficients:
##                Estimate Std. Error t value             Pr(&amp;gt;|t|)    
## (Intercept)   10.509475   0.052096 201.734 &amp;lt; 0.0000000000000002 ***
## acc_year       0.204224   0.021608   9.451     0.00000000000104 ***
## acc_year_2    -0.018295   0.002058  -8.891     0.00000000000719 ***
## dev_year_m1   -0.364073   0.008845 -41.160 &amp;lt; 0.0000000000000002 ***
## dev_year_ge_7  0.238860   0.088426   2.701              0.00941 ** 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 242.0614)
## 
##     Null deviance: 750824  on 54  degrees of freedom
## Residual deviance:  11879  on 50  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Assuming the fit is satisfactory, our original model with 19 parmaeters has now been simplified to 5 parameters - much more parsimonious and robust.&lt;/li&gt;
&lt;li&gt;Let’s check the fit by dev_year to see.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get the dev_year fit under the new model and add to the data.table containing the factor level parameters
p1 &amp;lt;- glm_fit3$coeff_table[parameter == &amp;quot;dev_year_m1&amp;quot;, coeff_glm_fit3]
p2 &amp;lt;- glm_fit3$coeff_table[parameter == &amp;quot;dev_year_ge_7&amp;quot;, coeff_glm_fit3]
dt_dev_year[, coeff_glm_fit3 := p1*(dev_year-1) + p2*pmax(0, dev_year-7.5) ]


# make long for ggplot
dt_dev_year_plot &amp;lt;- melt(dt_dev_year, id.vars = &amp;quot;dev_year&amp;quot;, measure.vars = c(&amp;quot;coeff_glm_fit2&amp;quot;, &amp;quot;coeff_glm_fit3&amp;quot;), variable.name=&amp;quot;model&amp;quot;, value = &amp;quot;estimate&amp;quot;)

# remove the coeff_ from the model names
dt_dev_year_plot[, model := gsub(&amp;quot;coeff_&amp;quot;, &amp;quot;&amp;quot;, model, fixed=TRUE)]


ggplot(data=dt_dev_year_plot, aes(x=dev_year, y=estimate, colour=model)) +
    geom_line(size=2) +
    geom_point(size=4) +
    scale_colour_viridis_d(begin=0, end=0.5) +
    theme_bw() +
    ggtitle(&amp;quot;Accident year parameters&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-07-traditional-style-reserving-using-glms_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This looks good.&lt;/li&gt;
&lt;li&gt;However dev_year = 2 is a bit underfit in the latest model, so we can add something to improve this fit&lt;/li&gt;
&lt;li&gt;So refit and replot.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;msdata[, dev_year_eq_2 := as.integer(dev_year == 2)]

glm_fit4 &amp;lt;- glm(data = msdata, 
    family = quasipoisson(link = &amp;quot;log&amp;quot;),
    formula = &amp;quot;incremental ~ acc_year + acc_year_2 + dev_year_m1 + dev_year_ge_7 + dev_year_eq_2&amp;quot;)


glm_fit4$coeff_table &amp;lt;- data.table(parameter = names(glm_fit4$coefficients), coeff_glm_fit4 = glm_fit4$coefficients)


p1 &amp;lt;- glm_fit4$coeff_table[parameter == &amp;quot;dev_year_m1&amp;quot;, coeff_glm_fit4]
p2 &amp;lt;- glm_fit4$coeff_table[parameter == &amp;quot;dev_year_ge_7&amp;quot;, coeff_glm_fit4]
p3 &amp;lt;- glm_fit4$coeff_table[parameter == &amp;quot;dev_year_eq_2&amp;quot;, coeff_glm_fit4]
dt_dev_year[, coeff_glm_fit4 := p1*(dev_year-1) + p2*pmax(0, dev_year-7.5) + p3*(dev_year == 2) ]


# make long for ggplot
dt_dev_year_plot &amp;lt;- melt(dt_dev_year, id.vars = &amp;quot;dev_year&amp;quot;, measure.vars = c(&amp;quot;coeff_glm_fit2&amp;quot;, &amp;quot;coeff_glm_fit4&amp;quot;), variable.name=&amp;quot;model&amp;quot;, value = &amp;quot;estimate&amp;quot;)

# remove the coeff_ from the model names
dt_dev_year_plot[, model := gsub(&amp;quot;coeff_&amp;quot;, &amp;quot;&amp;quot;, model, fixed=TRUE)]


ggplot(data=dt_dev_year_plot, aes(x=dev_year, y=estimate, colour=model)) +
    geom_line(size=2) +
    geom_point(size=4) +
    scale_colour_viridis_d(begin=0, end=0.5) +
    theme_bw() +
    ggtitle(&amp;quot;Accident year parameters&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-07-traditional-style-reserving-using-glms_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Looks good!&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;identifying-missing-structure&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Identifying missing structure&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The second part of the model refining process involves checking for missing structure.&lt;/li&gt;
&lt;li&gt;Let’s have a better look at the heat map.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;msdata[, residuals4 := rstandard(glm_fit4)
       ][, fitted4 := glm_fit4$fitted.values
         ][, linear_predictor4 := log(fitted4)
           ][, AvsF4 := incremental / fitted4
             ][, AvsF_restricted4 := log(pmax(0.5, pmin(2,AvsF4)))]


p_hm &amp;lt;- ggplot(data=msdata, aes(x=dev_year, y=acc_year)) + 
    geom_tile(aes(fill = AvsF_restricted4))+scale_y_reverse()+
    scale_fill_gradient2(name=&amp;quot;AvF_min&amp;quot;, low=&amp;quot;royalblue&amp;quot;, mid=&amp;quot;white&amp;quot;, high=&amp;quot;red&amp;quot;, midpoint=0, space=&amp;quot;Lab&amp;quot;, na.value=&amp;quot;grey50&amp;quot;, guide=&amp;quot;colourbar&amp;quot;)+
    labs(x=&amp;quot;Development year&amp;quot;, y=&amp;quot;Accident year&amp;quot;)+
    theme(legend.position = &amp;quot;none&amp;quot;)+
    theme(axis.title.x = element_text(size=8), axis.text.x  = element_text(size=7))+
    theme(axis.title.y = element_text(size=8), axis.text.y  = element_text(size=7))+
    theme(panel.background = element_rect(fill = &amp;quot;grey&amp;quot;, colour = &amp;quot;grey&amp;quot;, size = 2, linetype = &amp;quot;solid&amp;quot;),
          panel.grid = element_line(colour=&amp;quot;grey&amp;quot;)) + 
    NULL

print(p_hm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-07-traditional-style-reserving-using-glms_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s look at the heatmap again, with some annotations&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_hm + 
    annotate(geom=&amp;quot;rect&amp;quot;, xmin= 0.5, xmax=1.5, ymin=0.5, ymax=6.5, colour=&amp;quot;darkblue&amp;quot;, alpha=0.1, size=1.5) +
    annotate(geom=&amp;quot;rect&amp;quot;, xmin= 0.5, xmax=1.5, ymin=6.5, ymax=10.5, colour=&amp;quot;darkred&amp;quot;, alpha=0.1, size=1.5) +
    annotate(geom=&amp;quot;rect&amp;quot;, xmin= 1.5, xmax=2.5, ymin=0.5, ymax=6.5, colour=&amp;quot;darkred&amp;quot;, alpha=0.1, size=1.5) +
    annotate(geom=&amp;quot;rect&amp;quot;, xmin= 1.5, xmax=2.5, ymin=6.5, ymax=9.5, colour=&amp;quot;darkblue&amp;quot;, alpha=0.1, size=1.5) +
    annotate(geom=&amp;quot;segment&amp;quot;, x=3, xend=3, y=1, yend=8, arrow=arrow(), colour=&amp;quot;darkblue&amp;quot;, size=2) +
    annotate(geom=&amp;quot;rect&amp;quot;, xmin= 3.5, xmax=4.5, ymin=0.5, ymax=7.5, colour=&amp;quot;darkred&amp;quot;, alpha=0.1, size=1.5) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-07-traditional-style-reserving-using-glms_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;development year 1, a distinct area of blue in the earlier accident years (A &amp;lt; F), followed by red (A &amp;gt; F)&lt;/li&gt;
&lt;li&gt;development year 2, a distinct area of red in the earlier accident years (A &amp;gt; F), followed by blue (A &amp;lt; F)&lt;/li&gt;
&lt;li&gt;development year 3, a possible progression from red to blue with increasing accident year (F increasing relative to A)&lt;/li&gt;
&lt;li&gt;development year 4, nearly all red (A &amp;gt; F)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This suggests the payment pattern has altered and can be accommodated by (mostly) interaction terms within the GLM. Consider adding the following terms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(development year = 1) * (accident year is between 1 and 6)&lt;/li&gt;
&lt;li&gt;(development year = 2) * (accident year is between 1 and 6)&lt;/li&gt;
&lt;li&gt;(development year = 3) * (accident year linear trend)&lt;/li&gt;
&lt;li&gt;(development year = 4)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, let’s refit the model with terms to capture these and have a look at the heat map again&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# add the new terms
msdata[, dev_year_eq_1 := as.integer(dev_year == 1)]
msdata[, dev_year_eq_3 := as.integer(dev_year == 3)]
msdata[, dev_year_eq_4 := as.integer(dev_year == 4)]
msdata[, acc_year_1_6 := as.integer(acc_year &amp;gt;= 1 &amp;amp; acc_year &amp;lt;= 6)]


glm_fit5 &amp;lt;- glm(data = msdata, 
    family = quasipoisson(link = &amp;quot;log&amp;quot;),
    formula = &amp;quot;incremental ~ acc_year + acc_year_2 + dev_year_m1 + dev_year_ge_7 + dev_year_eq_2 + dev_year_eq_4 +
    dev_year_eq_1:acc_year_1_6 +  dev_year_eq_2:acc_year_1_6 + dev_year_eq_3:acc_year &amp;quot;)



glm_fit5$coeff_table &amp;lt;- data.table(parameter = names(glm_fit5$coefficients), coeff_glm_fit5 = glm_fit5$coefficients)

# print the coefficient table

glm_fit5$coeff_table %&amp;gt;% 
    knitr::kable(digits=c(0, 4))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;parameter&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;coeff_glm_fit5&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.4904&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;acc_year&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2066&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;acc_year_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0183&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dev_year_m1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.3685&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dev_year_ge_7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2720&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dev_year_eq_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0375&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dev_year_eq_4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dev_year_eq_1:acc_year_1_6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0671&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dev_year_eq_2:acc_year_1_6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1273&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;acc_year:dev_year_eq_3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0113&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This model should match that displayed in Table 7-5 of the monograph - and indeed it does (some very minor differences in parameter values - the model in the monograph was fitted in SAS).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Look at the heat map again with annotations - has the model resolved the identified issues?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# attach fitteds and residuals
msdata[, residuals5 := rstandard(glm_fit5)
       ][, fitted5 := glm_fit5$fitted.values
         ][, linear_predictor5 := log(fitted5)
           ][, AvsF5 := incremental / fitted5
             ][, AvsF_restricted5 := log(pmax(0.5, pmin(2,AvsF5)))]



p_hm &amp;lt;- ggplot(data=msdata, aes(x=dev_year, y=acc_year)) + 
    geom_tile(aes(fill = AvsF_restricted5))+scale_y_reverse()+
    scale_fill_gradient2(name=&amp;quot;AvF_min&amp;quot;, low=&amp;quot;royalblue&amp;quot;, mid=&amp;quot;white&amp;quot;, high=&amp;quot;red&amp;quot;, midpoint=0, space=&amp;quot;Lab&amp;quot;, na.value=&amp;quot;grey50&amp;quot;, guide=&amp;quot;colourbar&amp;quot;)+
    labs(x=&amp;quot;Development year&amp;quot;, y=&amp;quot;Accident year&amp;quot;)+
    theme(legend.position = &amp;quot;none&amp;quot;)+
    theme(axis.title.x = element_text(size=8), axis.text.x  = element_text(size=7))+
    theme(axis.title.y = element_text(size=8), axis.text.y  = element_text(size=7))+
    theme(panel.background = element_rect(fill = &amp;quot;grey&amp;quot;, colour = &amp;quot;grey&amp;quot;, size = 2, linetype = &amp;quot;solid&amp;quot;),
          panel.grid = element_line(colour=&amp;quot;grey&amp;quot;)) + 
    annotate(geom=&amp;quot;rect&amp;quot;, xmin= 0.5, xmax=1.5, ymin=0.5, ymax=6.5, colour=&amp;quot;darkblue&amp;quot;, alpha=0.1, size=1.5) +
    annotate(geom=&amp;quot;rect&amp;quot;, xmin= 0.5, xmax=1.5, ymin=6.5, ymax=10.5, colour=&amp;quot;darkred&amp;quot;, alpha=0.1, size=1.5) +
    annotate(geom=&amp;quot;rect&amp;quot;, xmin= 1.5, xmax=2.5, ymin=0.5, ymax=6.5, colour=&amp;quot;darkred&amp;quot;, alpha=0.1, size=1.5) +
    annotate(geom=&amp;quot;rect&amp;quot;, xmin= 1.5, xmax=2.5, ymin=6.5, ymax=9.5, colour=&amp;quot;darkblue&amp;quot;, alpha=0.1, size=1.5) +
    annotate(geom=&amp;quot;segment&amp;quot;, x=3, xend=3, y=1, yend=8, arrow=arrow(), colour=&amp;quot;darkblue&amp;quot;, size=2) +
    annotate(geom=&amp;quot;rect&amp;quot;, xmin= 3.5, xmax=4.5, ymin=0.5, ymax=7.5, colour=&amp;quot;darkred&amp;quot;, alpha=0.1, size=1.5) 


print(p_hm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-07-traditional-style-reserving-using-glms_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This looks much better.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We should also look at the residual plots again&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- ggplot(data=msdata, aes(x=linear_predictor5, y=residuals5, colour=dev_year)) +
    geom_point(size=2) +
    scale_colour_viridis(begin=0.9, end=0) +
    theme_bw() + 
    theme(legend.position = &amp;quot;none&amp;quot;) +
    ggtitle(&amp;quot;Linear predictor&amp;quot;)


p2 &amp;lt;- ggplot(data=msdata, aes(x=acc_year, y=residuals5)) +
    geom_point(size=2, colour=&amp;quot;#2d708eff&amp;quot;) +
    theme_bw() + 
    ggtitle(&amp;quot;Accident year&amp;quot;)

p3 &amp;lt;- ggplot(data=msdata, aes(x=dev_year, y=residuals5)) +
    geom_point(size=2, colour=&amp;quot;#2d708eff&amp;quot;) +
    theme_bw() + 
    ggtitle(&amp;quot;Development year&amp;quot;)

p4 &amp;lt;- ggplot(data=msdata, aes(x=cal_year, y=residuals5)) +
    geom_point(size=2, colour=&amp;quot;#2d708eff&amp;quot;) +
    theme_bw() + 
    ggtitle(&amp;quot;Calendar year&amp;quot;)

p &amp;lt;- plot_grid(p1, p2, p3, p4, nrow=2, rel_widths = c(1,1,1,1))

p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-07-traditional-style-reserving-using-glms_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;loss-reserve-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Loss reserve&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Now that we have a model, let’s produce the estimate of the outstanding claims by accident year and in total.
&lt;ul&gt;
&lt;li&gt;Take the lower triangle data [futdata] created above&lt;/li&gt;
&lt;li&gt;Add on the new variates we created&lt;/li&gt;
&lt;li&gt;Score the model on this data&lt;/li&gt;
&lt;li&gt;Summarise the results&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;
Create the data and score using &lt;code&gt;predict&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# add all model variates
futdata[, acc_year_2 := acc_year^2
        ][, dev_year_m1 := dev_year - 1
          ][, dev_year_ge_7 := pmax(0, dev_year - 7.5)
            ][, dev_year_eq_1 := as.integer(dev_year == 1)
              ][, dev_year_eq_2 := as.integer(dev_year == 2)
                ][, dev_year_eq_3 := as.integer(dev_year == 3)
                  ][, dev_year_eq_4 := as.integer(dev_year == 4)
                    ][, acc_year_1_6 := as.integer(acc_year&amp;gt;=1 &amp;amp; acc_year &amp;lt;=6)]


x &amp;lt;- predict(glm_fit5, newdata = futdata, type=&amp;quot;response&amp;quot;)
futdata[, incremental := x]

head(futdata)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    acc_year dev_year cal_year acc_year_factor dev_year_factor incremental
## 1:        2       10       12               2              10    3618.769
## 2:        3        9       12               3               9    4470.907
## 3:        3       10       13               3              10    4059.635
## 4:        4        8       12               4               8    5324.841
## 5:        4        9       13               4               9    4835.016
## 6:        4       10       14               4              10    4390.250
##    acc_year_2 dev_year_m1 dev_year_ge_7 dev_year_eq_1 dev_year_eq_2
## 1:          4           9           2.5             0             0
## 2:          9           8           1.5             0             0
## 3:          9           9           2.5             0             0
## 4:         16           7           0.5             0             0
## 5:         16           8           1.5             0             0
## 6:         16           9           2.5             0             0
##    dev_year_eq_3 dev_year_eq_4 acc_year_1_6
## 1:             0             0            1
## 2:             0             0            1
## 3:             0             0            1
## 4:             0             0            1
## 5:             0             0            1
## 6:             0             0            1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Get reserves by accident year and in total&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ocl_year &amp;lt;- futdata[,  lapply(.SD, sum), .SDcols=c(&amp;quot;incremental&amp;quot;), by=&amp;quot;acc_year&amp;quot;]
ocl_total &amp;lt;- ocl_year[, sum(incremental)]


ocl_year %&amp;gt;% 
    knitr::kable(digits=c(0, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;acc_year&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;incremental&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3619&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8531&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14550&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22173&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32458&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;45695&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;62955&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;79301&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;101212&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The total reserve is&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ocl_total %&amp;gt;% round(0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 370493&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;These results are similar, though not identical, to the results given in Table 7-6 of the monograph.&lt;/li&gt;
&lt;li&gt;This is because the &lt;em&gt;forecast&lt;/em&gt; column of the monograph contains bootstrapped means rather than the model mean.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The aim of this article has been to demonstrate fitting a GLM to a loss reserve following the example used in the CAS monograph.
We started with the chain ladder equivalent - the cross classified model with an over-dispersed Poisson distribution, then first simplifed it and second, extended it to include some interactions.
We also cover how to create some of the plots discussed in the monograph in R, in particular residual scatter plots and the heat maps.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-information&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Session information&lt;/h2&gt;
&lt;p&gt;To assist with reproducibility, here are details of my R session.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 3.6.1 (2019-07-05)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 18363)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_Australia.1252  LC_CTYPE=English_Australia.1252   
## [3] LC_MONETARY=English_Australia.1252 LC_NUMERIC=C                      
## [5] LC_TIME=English_Australia.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] magrittr_1.5      cowplot_1.0.0     viridis_0.5.1     viridisLite_0.3.0
## [5] ggplot2_3.2.1     data.table_1.12.8 here_0.1         
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.3       highr_0.8        compiler_3.6.1   pillar_1.4.2    
##  [5] tools_3.6.1      digest_0.6.23    evaluate_0.14    lifecycle_0.1.0 
##  [9] tibble_2.1.3     gtable_0.3.0     pkgconfig_2.0.3  rlang_0.4.2     
## [13] yaml_2.2.0       blogdown_0.17    xfun_0.11        gridExtra_2.3   
## [17] withr_2.1.2      stringr_1.4.0    dplyr_0.8.3      knitr_1.26      
## [21] rprojroot_1.3-2  grid_3.6.1       tidyselect_0.2.5 glue_1.3.1      
## [25] R6_2.4.1         rmarkdown_2.0    bookdown_0.17    farver_2.0.1    
## [29] purrr_0.3.3      backports_1.1.5  scales_1.1.0     htmltools_0.4.0 
## [33] assertthat_0.2.1 colorspace_1.4-1 labeling_0.3     stringi_1.4.3   
## [37] lazyeval_0.2.2   munsell_0.5.0    crayon_1.3.4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Self-assembling claim reserving models</title>
      <link>/post/self-assembling-claim-reserving-models/</link>
      <pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate>
      <guid>/post/self-assembling-claim-reserving-models/</guid>
      <description>


&lt;!-- output formats in the _output.yml file --&gt;
&lt;p&gt;Recently, Greg Taylor, Hugh Miller and myself completed some work to develop an approach for fitting a claims reserving model using regularised regression, the LASSO in particular. See the &lt;a href=&#34;/publication/sagacious&#34;&gt;SSRN paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The motivation behind this work is to develop an automatic method to construct a general insurance claims reserving model for data sets with complex features where simple approaches such as the chain ladder fail. In the past we have advocated the use of GLM models for such data sets, but the construction of a GLM is a time-consuming process, even for a skilled analyst. Our aim was to develop a procedure that produced a model similar to a GLM, but using machine learning techniques. This approach has performed well - as may be seen from the charts towards the end of this example, which show that the fitted curves can track the underlying specification quite well, even in the presence of significant noise, or difficult to detect interactions.&lt;/p&gt;
&lt;p&gt;The paper fills in the details around the approach, so they will not be repeated here. Instead, this will focus on illustrating the use of the LASSO in claims reserving by fitting the model to one of the synthetic data examples discussed in the paper. I will be working through a fully worked example with all R code available. To reduce dependencies on external libraries, I have carried out the work in base R as much as possible.&lt;/p&gt;
&lt;p&gt;If you want to access the underlying Rnotebook directly, you may access it at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/grainnemcguire/code_snippets/blob/master/R/2019-05-31-self-assembling-claim-reserving-models.Rmd&#34;&gt;2019-05-31-self-assembling-claim-reserving-models.Rmd&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;introduction-to-the-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction to the example&lt;/h1&gt;
&lt;p&gt;In our paper we investigate the use of the LASSO using four synthetic (i.e. simulated) data sets as well as a real data set. This worked example will use the third simulated data set. the specifications for this data set are given in Section 4.2.1 of the paper. The main points to note are that this data set :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;is a 40x40 triangle, with&lt;/li&gt;
&lt;li&gt;accident and development period effects&lt;/li&gt;
&lt;li&gt;calendar period effects (i.e. superimposed inflation)&lt;/li&gt;
&lt;li&gt;an step-interaction between accident and development period for accident periods greater than 17 and development periods greater than 20 (note this affects only 10 cells of the triangle)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;r-code&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R code&lt;/h1&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;First we must open R and load any libraries required. As noted above, I will use base R as much as possible so there are only two additional libraries required:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;glmnet&lt;/code&gt; - this is the library we used to implement the LASSO model. References are in our paper.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ggplot2&lt;/code&gt; - for producing graphs.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(glmnet)
## Loading required package: Matrix
## Loaded glmnet 3.0-2
library(ggplot2)

options(&amp;quot;scipen&amp;quot;=99)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;generating-the-synthetic-data-set&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generating the synthetic data set&lt;/h2&gt;
&lt;p&gt;First we need a data set. The code below, using a seed of 130 produces the data set used in the paper.&lt;/p&gt;
&lt;p&gt;Before we generate the data, we need to create a small utility function which will be widely used - it takes a vector &lt;em&gt;var&lt;/em&gt; and produces a spline piece between &lt;code&gt;start&lt;/code&gt; and &lt;code&gt;stop&lt;/code&gt; - flat (and 0) up to &lt;code&gt;start&lt;/code&gt;, thereafter increasing to &lt;code&gt;stop&lt;/code&gt; and then levelling out at this point. This is used both in the data generation and in the generation of basis functions for the LASSO.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LinearSpline &amp;lt;- function(var, start, stop){
    pmin(stop - start, pmax(0, var - start))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code below generates the data set as specified in our paper. If you want to use our data set, use a seed of 130. Otherwise, use different seeds to produce different data sets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# initialise the seed for reproducibilty
set.seed(130)
numperiods &amp;lt;- 40

#periods
kk &amp;lt;- rep(1:numperiods, each = numperiods) #AQ
jj &amp;lt;- rep(1:numperiods, times= numperiods) #DQ
tt &amp;lt;- kk+jj-1 # PQ


# make a function of the SI effect to make it easier to read 
gammafunc &amp;lt;- function(t){
    gg &amp;lt;- 
        ifelse( t&amp;lt;=12, gg &amp;lt;- 0.0075*LinearSpline(t,1,12),
                ifelse(t&amp;lt;=24,  gg &amp;lt;- 0.0075*LinearSpline(12,1,12) + 0.001* (t-12)*(t-11)/2,
                       ifelse(t&amp;lt;=32, gg &amp;lt;- 0.0075*LinearSpline(12,1,12) + 0.001* (24-12)*(24-11)/2,
                           ifelse(t&amp;lt;=40, gg &amp;lt;- 0.0075*LinearSpline(12,1,12) + 0.001* (24-12)*(24-11)/2 + 0.002*(t-32)*(t-31)/2,
                               0.0075*LinearSpline(12,1,12) + 0.001* (24-12)*(24-11)/2 + 0.002*(40-32)*(40-31)/2
                           ))))
    1*gg  #can scale up shape here if desired
}

alpha &amp;lt;- log(100000)+0.1*LinearSpline(kk,1,15)+0.2*LinearSpline(kk,15,20) - 0.05*LinearSpline(kk,30,40) 
beta  &amp;lt;- (16/3 - 1)*log(jj)- (1/3)*jj  # a is 16/3, b is 1/3 
gamma &amp;lt;- gammafunc(tt)
mu &amp;lt;- exp( alpha + beta + gamma + 0.3*beta*ifelse(kk&amp;gt;16 &amp;amp; jj&amp;gt;20,1,0))  # need to check

varbase &amp;lt;- (0.3 * mu[  kk==1 &amp;amp; jj ==16] )^2 # can scale variance up and down here
CC  &amp;lt;-  varbase / mu[  kk==1 &amp;amp; jj ==16]

vars   &amp;lt;- CC*mu
tausq  &amp;lt;- log (vars / (mu^2) + 1)

Y &amp;lt;- exp( rnorm( numperiods^2, mean = log(mu)-0.5*tausq , sd = sqrt(tausq)  ) )

train_ind&amp;lt;-(tt&amp;lt;=numperiods)  

synthetic_data&amp;lt;-data.frame(Y, kk, jj, tt, mu, train_ind )
colnames(synthetic_data)&amp;lt;-c(&amp;quot;Y&amp;quot;, &amp;quot;acc&amp;quot;, &amp;quot;dev&amp;quot;, &amp;quot;cal&amp;quot;, &amp;quot;mu&amp;quot;, &amp;quot;train_ind&amp;quot;)


# clean-up
rm(Y, kk, jj, tt, mu, train_ind)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;
Let’s have a look at the data&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(synthetic_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Y acc dev cal          mu train_ind
## 1   242671.2   1   1   1    71653.13      TRUE
## 2   164001.3   1   2   2  1042775.62      TRUE
## 3  3224477.8   1   3   3  4362599.77      TRUE
## 4  3682530.8   1   4   4 10955670.09      TRUE
## 5 10149368.6   1   5   5 20800545.12      TRUE
## 6 28578274.7   1   6   6 33089166.75      TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(synthetic_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Y acc dev cal        mu train_ind
## 1595 125261750  40  35  74 109039367     FALSE
## 1596  62657370  40  36  75  82853302     FALSE
## 1597  63467681  40  37  76  62682720     FALSE
## 1598  26041979  40  38  77  47227843     FALSE
## 1599  33947274  40  39  78  35444881     FALSE
## 1600  37258687  40  40  79  26503298     FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The points to note about this data frame are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Y&lt;/code&gt; is the comumn containing the data - assumed to be incremental claims payments here&lt;/li&gt;
&lt;li&gt;&lt;code&gt;acc&lt;/code&gt;, &lt;code&gt;dev&lt;/code&gt; and &lt;code&gt;cal&lt;/code&gt; are the columns with the accident, development and calendar period labels. Note that development period is numbered from 1.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mu&lt;/code&gt; contains the values for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, the true underlying mean value according to the model specifications&lt;/li&gt;
&lt;li&gt;&lt;code&gt;train_ind&lt;/code&gt; is a TRUE/FALSE vector. It is TRUE when the observation is in the past, a.k.a. the training data set and FALSE for future observations (unlike real data, we have the future available for a simulated data set)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;specifying-the-lasso-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Specifying the LASSO model&lt;/h2&gt;
&lt;p&gt;The LASSO model requires data (which we have), basis functions or regressors and model settings for the LASSO procedure. Unlike many regression problems, we actually only have 3 fundamental regressors - accident, development and calendar periods. A key part of our paper was how to expand these into a flexible set of basis functions, capable of capturing a variety of shapes in the model experience.&lt;/p&gt;
&lt;p&gt;In a nutshell, the approach in our paper sets out:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the use of ramp and interactions of indicator (or heaviside) functions to capture a range of experience&lt;/li&gt;
&lt;li&gt;scaling factors for each of these.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since our process is intended to be automatic, we included all functions of accident, development and calendar periods into our synthetic data models, even though that introduces correlations between variables, and examined performance on that basis. For real problems (as in the real data example in the paper) we would recommend more discernment in the selection of which of the effects to include.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;scaling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Scaling&lt;/h3&gt;
&lt;p&gt;For now, we are working to replicate the model for synthetic data set 3 so we include all possible functions (and indeed, this data set has accident, development and calendar period effects as well as an interaction). The first step is to calculate the scaling factors for each function derived from each of the three fundamental regressors. As per the paper we use &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; scaling which is calculated as &lt;span class=&#34;math inline&#34;&gt;\(\sum{\frac{(x-\bar{x})^2}{n}}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; is the mean of the regressor &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of elements in the regressor.&lt;/p&gt;
&lt;p&gt;For this synthetic data set, it is important to remember that in the real world, the future data will not be available. So the fundamental vectors are those containing past (or training) data only.&lt;/p&gt;
&lt;p&gt;We also need to calculate the scaling for each of the three regressors, so for coding efficiency and to reduce the risk of bugs, we first write a little function to do the calculations.&lt;/p&gt;
&lt;p&gt;Note that this scaling is a crucial component to the successful implementation of the reserving LASSO. The paper has more details, but basically the size of the parameters has a significant influence on whether they are included in a regularised regression, so if basis functions are on different scales, then this will influence their inclusion in the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# function for ease
GetScaling &amp;lt;- function(vec) {
  fn &amp;lt;- length(vec)
  fm &amp;lt;- mean(vec)
  fc &amp;lt;- vec - fm
  rho_factor &amp;lt;- ((sum(fc^2))/fn)^0.5
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we apply this function to each of the three fundamental regressors (limited to the training or past data) and store the results in a list.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;synthetic_data_train&amp;lt;-subset(synthetic_data, train_ind==TRUE)

rho_factor_list &amp;lt;- vector(mode=&amp;quot;list&amp;quot;, length=3)
names(rho_factor_list) &amp;lt;- c(&amp;quot;acc&amp;quot;, &amp;quot;dev&amp;quot;, &amp;quot;cal&amp;quot;)

for (v in c(&amp;quot;acc&amp;quot;, &amp;quot;dev&amp;quot;, &amp;quot;cal&amp;quot;)){
  rho_factor_list[[v]] &amp;lt;- GetScaling(synthetic_data_train[[v]])
}


print(rho_factor_list)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $acc
## [1] 9.539392
## 
## $dev
## [1] 9.539392
## 
## $cal
## [1] 9.539392&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The factors are all equal in this case. In hindsight this makes sense - we have the complete 40 x 40 past triangle, and all regressors are numbered from 1, so all three regressors are permutations of the same numbers.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;basis-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Basis functions&lt;/h3&gt;
&lt;p&gt;Our recommended set of basis functions include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ramp functions for main effects:
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(R_j(i) = \max(j-i, 0)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(j=\)&lt;/span&gt; accident, development and calendar periods and &lt;span class=&#34;math inline&#34;&gt;\(i=1, 2, ..., 39\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;interactions of the heaviside indicator functions:
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_j(i) = I(j&amp;gt;=i)\)&lt;/span&gt;, i.e. all combinations of &lt;span class=&#34;math inline&#34;&gt;\(H_{acc}(i) * H_{dev}(k)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(H_{acc}(i) * H_{cal}(k)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(H_{dev}(i) * H_{cal}(k)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;main-effects---ramp-functions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Main effects - ramp functions&lt;/h4&gt;
&lt;p&gt;To calculate the ramps we first write a function to do this, since we need to repeat this three times. glmnet expects a matrix of values for the basis functions, so we create a matrix rather than a data.frame. Furthermore, to ensure speed, we preallocate the space to store all the basis functions before running a loop to produce all the ramp functions (many people are scared of loops in R finding them slow, but often this is because they do not preallocate memory beforehand to store the results of the loop)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GetRamps &amp;lt;- function(vec, vecname, np, scaling){
  
  # vec = fundamental regressor
  # vecname = name of regressor
  # np = number of periods
  # scaling = scaling factor to use
  
  # pre-allocate the matrix to hold the results for speed/efficiency
  n &amp;lt;- length(vec)
  nramps &amp;lt;- (np-1)
  
  mat &amp;lt;- matrix(data=NA, nrow=n, ncol=nramps)
  cnames &amp;lt;- vector(mode=&amp;quot;character&amp;quot;, length=nramps)
  

  col_indx &amp;lt;- 0

  for (i in 1:(np-1)){
    col_indx &amp;lt;- col_indx + 1

    mat[, col_indx] &amp;lt;- LinearSpline(vec, i, 999) / scaling
    cnames[col_indx] &amp;lt;- paste0(&amp;quot;L_&amp;quot;, i, &amp;quot;_999_&amp;quot;, vecname)
  }
  
  colnames(mat) &amp;lt;- cnames
  
  return(mat)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s run the function 3 times to get each set of ramp functions and combine them at the end. Note that we produce ramps for the &lt;strong&gt;entire&lt;/strong&gt; data set, past and future. This leads to a 1600 x 117 (39*3) matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;main_effects_acc &amp;lt;- GetRamps(vec = synthetic_data[[&amp;quot;acc&amp;quot;]], vecname = &amp;quot;acc&amp;quot;, 
                             np = numperiods, scaling = rho_factor_list[[&amp;quot;acc&amp;quot;]])
main_effects_dev &amp;lt;- GetRamps(vec = synthetic_data[[&amp;quot;dev&amp;quot;]], vecname = &amp;quot;dev&amp;quot;, 
                             np = numperiods, scaling = rho_factor_list[[&amp;quot;dev&amp;quot;]])
main_effects_cal &amp;lt;- GetRamps(vec = synthetic_data[[&amp;quot;cal&amp;quot;]], vecname = &amp;quot;cal&amp;quot;, 
                             np = numperiods, scaling = rho_factor_list[[&amp;quot;cal&amp;quot;]])
    
main_effects &amp;lt;- cbind(main_effects_acc, main_effects_dev, main_effects_cal)

print(dim(main_effects))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1600  117&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;interaction-effects---heaviside-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interaction effects - heaviside functions&lt;/h3&gt;
&lt;p&gt;We follow a similar approach to the above - write a function since we need to call it 3 times. It is even more important to preallocate memory here before looping to create interactions because of the number of these. Without this, the loop would need to copy an ever-growing matrix each time, thereby significantly slowing down runtimes. Our approach of slotting values into reserved space is far more efficient.&lt;/p&gt;
&lt;p&gt;Here’s the function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GetInts &amp;lt;- function(vec1, vec2, vecname1, vecname2, np, scaling1, scaling2) {
  
  # pre-allocate the matrix to hold the results for speed/efficiency
  n &amp;lt;- length(vec1)
  nints &amp;lt;- (np-1)*(np-1)
  
  mat &amp;lt;- matrix(data=NA_real_, nrow=n, ncol=nints)
  cnames &amp;lt;- vector(mode=&amp;quot;character&amp;quot;, length=nints)
  

  col_indx &amp;lt;- 0

  for (i in 2:np){
    
    ivec &amp;lt;- LinearSpline(vec1, i-1, i) / scaling1
    iname &amp;lt;- paste0(&amp;quot;I_&amp;quot;, vecname1, &amp;quot;_ge_&amp;quot;, i)
    
    if (length(ivec[is.na(ivec)]&amp;gt;0)) print(paste(&amp;quot;NAs in ivec for&amp;quot;, i))
    
    for (j in 2:np){
      col_indx &amp;lt;- col_indx + 1  
      mat[, col_indx] &amp;lt;- ivec * LinearSpline(vec2, j-1, j) / scaling2
      cnames[col_indx] &amp;lt;- paste0(iname, &amp;quot;*I_&amp;quot;, vecname2, &amp;quot;_ge_&amp;quot;, j)
      
      jvec &amp;lt;- LinearSpline(vec2, j-1, j) / scaling2
      if (length(jvec[is.na(jvec)]&amp;gt;0)) print(paste(&amp;quot;NAs in jvec for&amp;quot;, j))

    }
  }
  
  colnames(mat) &amp;lt;- cnames
  
  return(mat)

  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we call it and check the dimensions - 1600 x 4563(! 3 * 39 * 39).:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;int_effects &amp;lt;- cbind(
  GetInts(vec1=synthetic_data[[&amp;quot;acc&amp;quot;]], vecname1=&amp;quot;acc&amp;quot;, scaling1=rho_factor_list[[&amp;quot;acc&amp;quot;]], np=numperiods, 
                       vec2=synthetic_data[[&amp;quot;dev&amp;quot;]], vecname2=&amp;quot;dev&amp;quot;, scaling2=rho_factor_list[[&amp;quot;dev&amp;quot;]]),

    GetInts(vec1=synthetic_data[[&amp;quot;dev&amp;quot;]], vecname1=&amp;quot;dev&amp;quot;, scaling1=rho_factor_list[[&amp;quot;dev&amp;quot;]], np=numperiods, 
                       vec2=synthetic_data[[&amp;quot;cal&amp;quot;]], vecname2=&amp;quot;cal&amp;quot;, scaling2=rho_factor_list[[&amp;quot;cal&amp;quot;]]),
  
    GetInts(vec1=synthetic_data[[&amp;quot;acc&amp;quot;]], vecname1=&amp;quot;acc&amp;quot;, scaling1=rho_factor_list[[&amp;quot;acc&amp;quot;]], np=numperiods, 
                       vec2=synthetic_data[[&amp;quot;cal&amp;quot;]], vecname2=&amp;quot;cal&amp;quot;, scaling2=rho_factor_list[[&amp;quot;cal&amp;quot;]])

)


print(dim(int_effects))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1600 4563&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally combine the main and interactions effects into a single matrix - &lt;em&gt;varset&lt;/em&gt;. Also save a vector containing the training (past) data indicator and get the number of main and interaction effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;varset &amp;lt;- cbind(main_effects, int_effects)

train_ind &amp;lt;- synthetic_data[[&amp;quot;train_ind&amp;quot;]]

num_main_effects &amp;lt;- ncol(main_effects)
num_interactions &amp;lt;- ncol(int_effects)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lasso-setup&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;LASSO setup&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;glmnet&lt;/code&gt; package has two functions: &lt;code&gt;glmnet&lt;/code&gt; and &lt;code&gt;cv.glmnet&lt;/code&gt;. &lt;code&gt;glmnet&lt;/code&gt; fits a regularised regression model while &lt;code&gt;cv.glmnet&lt;/code&gt; fits using cross validation. We make use of both functions, first &lt;code&gt;glmnet&lt;/code&gt; to get a vector of LASSO penalties (referred to as &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;), then &lt;code&gt;cv.glmnet&lt;/code&gt; using the &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;s from &lt;code&gt;glmnet&lt;/code&gt; in the cross validation process.&lt;/p&gt;
&lt;p&gt;In addition to the data and basis functions, &lt;code&gt;glmnet&lt;/code&gt; and &lt;code&gt;cv.glmnet&lt;/code&gt; have a number of tuning parameters. These include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A set of values for &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, the regularision penalty. We use the &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; vector estimated by &lt;code&gt;glmnet&lt;/code&gt; and then extend it further to ensure that we have a comprehensive range of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values for the cross-validation exercise.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;penalty factors: this is a basis function specific penalty factor so provides the functionality to have different penalties for the different basis functions, i.e. if &lt;span class=&#34;math inline&#34;&gt;\(pf\)&lt;/span&gt; is the vector of penalty functions, then &lt;span class=&#34;math inline&#34;&gt;\(\lambda {pf}\)&lt;/span&gt; is the set of penalty factors used. Here we use the default of the same scaling for all functions (factors of 1 throughout), but we set up a penalty factor vector below in case you would like to experiment with it - e.g. to make interactions less or more likely to be used you could increase/decrease the penalty for interactions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;pmax&lt;/code&gt; - the maximum number of variables ever to be non-zero in a model&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;dfmax&lt;/code&gt; - maximum number of variables in a model&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;family&lt;/code&gt; - the response type to use. Of the options offered by the &lt;code&gt;glmnet&lt;/code&gt; package, the Poisson is the best selection for a claims reserving model.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The settings we used for the latter 3 are below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;penalty_factor &amp;lt;- c( rep(1,num_main_effects), rep(1, num_interactions))

my_pmax &amp;lt;- numperiods^2   # max number of variables ever to be nonzero
my_dfmax &amp;lt;- numperiods*10  #max number of vars in the model&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get the vector &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values to use in the cross validation, run &lt;code&gt;glmnet&lt;/code&gt; as below. Note that &lt;code&gt;alpha=1&lt;/code&gt; makes it fit a LASSO.
We have also increased the maximum number of iterations to 200000 since convergence can be slow in this example. The code takes about 13-15 sec on my computer.&lt;/p&gt;
&lt;p&gt;Note we set the &lt;code&gt;standardize&lt;/code&gt; argument to false - this is because we use our own standardisation for the basis functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;time1 &amp;lt;- Sys.time()

pre_fit &amp;lt;- glmnet(x = varset[train_ind,], 
                  y = synthetic_data$Y[train_ind], 
                  family = &amp;quot;poisson&amp;quot;, 
                  nlambda = 200, 
                  thresh = 1e-08, 
                  lambda.min.ratio = 0, 
                  dfmax = my_dfmax, 
                  pmax = my_pmax, 
                  alpha = 1, 
                  standardize = FALSE, 
                  penalty.factor = penalty_factor, 
                  maxit = 200000)

print(paste(&amp;quot;time taken: &amp;quot;, Sys.time() - time1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;time taken:  15.7549350261688&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; vector that we actually used in the cross validation implementation of the LASSO is an extended version of the one automatically generated by &lt;code&gt;glmnet&lt;/code&gt;. This is to ensure that we do find the minimum CV error point as sometimes it may be beyond the smallest value for &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; produced by &lt;code&gt;glmnet&lt;/code&gt; above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lambdavec &amp;lt;- c(pre_fit$lambda, min(pre_fit$lambda)*(0.85^(1:50)))  # lengthen lambda vector&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-the-lasso-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting the LASSO model&lt;/h2&gt;
&lt;p&gt;We now do the actual fitting using 8-fold cross validation (Rob Tibshirani, who wrote the original statistical paper on LASSO, recommends between 5 and 10 folds). We use &lt;code&gt;lambdavec&lt;/code&gt;, rather than letting &lt;code&gt;cv.glmnet&lt;/code&gt; estimate its own version. Otherwise the settings are the same.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#fit now using CV
time1 &amp;lt;- Sys.time()

cv_fit &amp;lt;- cv.glmnet(x = varset[train_ind,], 
                  y = synthetic_data$Y[train_ind], 
                  family = &amp;quot;poisson&amp;quot;, 
                  lambda = lambdavec, 
                  nfolds = 8,
                  thresh = 1e-08, 
                  lambda.min.ratio = 0, 
                  dfmax = my_dfmax, 
                  pmax = my_pmax, 
                  alpha = 1, 
                  standardize = FALSE, 
                  penalty.factor = penalty_factor, 
                  maxit = 200000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: from glmnet Fortran code (error code -10195); Number of nonzero
## coefficients along the path exceeds pmax=1600 at 195th lambda value; solutions
## for larger lambdas returned&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: from glmnet Fortran code (error code -10196); Number of nonzero
## coefficients along the path exceeds pmax=1600 at 196th lambda value; solutions
## for larger lambdas returned

## Warning: from glmnet Fortran code (error code -10196); Number of nonzero
## coefficients along the path exceeds pmax=1600 at 196th lambda value; solutions
## for larger lambdas returned

## Warning: from glmnet Fortran code (error code -10196); Number of nonzero
## coefficients along the path exceeds pmax=1600 at 196th lambda value; solutions
## for larger lambdas returned&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: from glmnet Fortran code (error code -10195); Number of nonzero
## coefficients along the path exceeds pmax=1600 at 195th lambda value; solutions
## for larger lambdas returned

## Warning: from glmnet Fortran code (error code -10195); Number of nonzero
## coefficients along the path exceeds pmax=1600 at 195th lambda value; solutions
## for larger lambdas returned

## Warning: from glmnet Fortran code (error code -10195); Number of nonzero
## coefficients along the path exceeds pmax=1600 at 195th lambda value; solutions
## for larger lambdas returned&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: from glmnet Fortran code (error code -10196); Number of nonzero
## coefficients along the path exceeds pmax=1600 at 196th lambda value; solutions
## for larger lambdas returned

## Warning: from glmnet Fortran code (error code -10196); Number of nonzero
## coefficients along the path exceeds pmax=1600 at 196th lambda value; solutions
## for larger lambdas returned&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste(&amp;quot;time taken for cross validation fit: &amp;quot;, Sys.time() - time1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;time taken for cross validation fit:  3.81268668572108&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;
&lt;em&gt;Eek! Fortran error&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Yes, there are Fortran errors. However, if you read the errors, you see that they result from our settings of the &lt;code&gt;pmax&lt;/code&gt; variable. Basically models for the 195th and higher values (and since the &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values monotonically decrease this means smaller values) in the &lt;code&gt;lambdavec&lt;/code&gt; do not return solutions that meet the constraints on &lt;code&gt;pmax&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Therefore the errors are not an issue as long as the cross validation process has found a minimum value within the range of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values it does consider. &lt;code&gt;cv.glmnet&lt;/code&gt; objects have a plot method associated with them which plots the CV fit, so I’ll use it here to see if a minimum value was identified:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(cv_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-31-self-assembling-claim-reserving-models_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The dashed lines represent the minimum CV model (smaller &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;) and one a standard deviation away. These are selections commonly used by modellers. So while we only got models for 194 of the original 250 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values input in &lt;code&gt;lambdavec&lt;/code&gt;, these models do include a model which produces the minimum CV error (this is actually that the 148th value in &lt;code&gt;lambdavec&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
Given this is simulated data, we can also look at test error - the graph below shows training, test and CV error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#training error
predicted_train &amp;lt;- exp(predict(cv_fit, newx = varset[train_ind,], s = cv_fit$lambda))
error_train &amp;lt;- colMeans(((synthetic_data$Y[train_ind] - predicted_train)^2) / predicted_train )

#test error - note 
predicted_test &amp;lt;- exp(predict(cv_fit, newx = varset[!train_ind,], s = cv_fit$lambda))
error_test &amp;lt;- colMeans(((synthetic_data$Y[!train_ind] - predicted_test)^2) / predicted_test )


# number of parameters in models (df field)
# since not all lambdas returned values, use length of cv.glmnet object to get the right value
use_df &amp;lt;- cv_fit$glmnet.fit$df[1:length(cv_fit$lambda)]

#make a stacked data set suitable for ggplot [ i.e. tidy format]
dferrorg &amp;lt;- data.frame( 
  rep(1:length(use_df), times=3), 
  rep(use_df, times=3), 
  c(error_train, error_test, cv_fit$cvm)/1000,
    c( rep(&amp;quot;Training error&amp;quot;, times=length(use_df)), rep(&amp;quot;Test error&amp;quot;, times=length(use_df)), rep(&amp;quot;CV error&amp;quot;, times=length(use_df)) )
)

colnames(dferrorg)&amp;lt;-c(&amp;quot;model_number&amp;quot;, &amp;quot;num_params&amp;quot;, &amp;quot;Error&amp;quot;, &amp;quot;labels&amp;quot;)  


g &amp;lt;- ggplot(data=dferrorg, aes(x=model_number, y=Error, colour=labels))+
    geom_line(size=1.5, aes(linetype=labels, colour=labels), alpha=0.8)+
    theme_classic()+
    theme(legend.position=&amp;quot;bottom&amp;quot;)+
    labs(x=&amp;quot;Model number&amp;quot;, y=&amp;quot;Error measure (thousands)&amp;quot;)+
    scale_color_manual(name=&amp;quot;&amp;quot;, values=c(&amp;quot;grey40&amp;quot;, &amp;quot;steelblue3&amp;quot;, &amp;quot;dodgerblue4&amp;quot;))+
    scale_linetype_manual(name=&amp;quot;&amp;quot;, values=c(&amp;quot;solid&amp;quot;, &amp;quot;dashed&amp;quot;, &amp;quot;dotdash&amp;quot;))+
    scale_y_log10(labels=scales::comma)+
    NULL

print(g)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-31-self-assembling-claim-reserving-models_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Happily, low values of the test error align with low CV error values - which is what we would expect to see.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;analysing-the-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analysing the model&lt;/h1&gt;
&lt;p&gt;We have used the model corresponding to the minimum CV error in the paper (&lt;code&gt;lambda.min&lt;/code&gt; in the &lt;code&gt;cv.glmnet&lt;/code&gt; results object, &lt;code&gt;cv_fit&lt;/code&gt;. First let’s look at the coefficients in this model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#all coefficients
coefs_min &amp;lt;- predict(cv_fit, type = &amp;quot;coefficients&amp;quot;, s = cv_fit$lambda.min)
coefnames &amp;lt;- c(&amp;quot;Intercept&amp;quot;, colnames(varset))

#indicators for non-zero ones
ind_nz_min&amp;lt;-which(!(coefs_min == 0))

#non-zero coefficient
nzcoefs_min&amp;lt;-cbind(coefs_min[ind_nz_min,])
rownames(nzcoefs_min)&amp;lt;-coefnames[ind_nz_min]
colnames(nzcoefs_min)&amp;lt;-c(&amp;quot;coefficients [min CV model]&amp;quot;)



print(paste(&amp;quot;Number of non-zero coefficients in min CV model:&amp;quot;, length(nzcoefs_min)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Number of non-zero coefficients in min CV model: 85&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(nzcoefs_min)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                         coefficients [min CV model]
## Intercept                            12.90849898487
## L_3_999_acc                          -0.17095448722
## L_6_999_acc                           0.20215486971
## L_8_999_acc                          -0.13683788579
## L_9_999_acc                          -0.00672219229
## L_10_999_acc                         -0.00726869090
## L_11_999_acc                         -0.02324636411
## L_12_999_acc                          0.35610585646
## L_14_999_acc                         -0.18739286935
## L_15_999_acc                          0.40181871769
## L_16_999_acc                          0.87558148156
## L_18_999_acc                         -0.51019660027
## L_19_999_acc                         -0.16864836308
## L_20_999_acc                         -1.43901702819
## L_21_999_acc                          0.00953486890
## L_23_999_acc                         -0.79245482450
## L_24_999_acc                          1.24495942278
## L_25_999_acc                         -0.93508758199
## L_26_999_acc                          0.33765206640
## L_30_999_acc                         -0.70157530992
## L_33_999_acc                          0.50643663606
## L_1_999_dev                           8.99462960013
## L_2_999_dev                          -0.01963809794
## L_3_999_dev                          -0.07441072105
## L_4_999_dev                          -3.07653114572
## L_5_999_dev                          -2.80431602371
## L_6_999_dev                          -0.68784566665
## L_7_999_dev                          -0.75804243590
## L_8_999_dev                          -0.98088683637
## L_9_999_dev                          -0.22339935171
## L_10_999_dev                         -0.29737982507
## L_11_999_dev                         -0.65592216016
## L_12_999_dev                         -0.35450909252
## L_13_999_dev                         -0.14473152847
## L_14_999_dev                          0.04054773673
## L_15_999_dev                         -0.77588436820
## L_17_999_dev                         -0.03182413168
## L_19_999_dev                          0.67859925074
## L_21_999_dev                         -1.60282884304
## L_22_999_dev                         -0.36525227453
## L_24_999_dev                          1.09988151243
## L_26_999_dev                         -1.09424403563
## L_27_999_dev                         -0.16350768634
## L_29_999_dev                          0.88266152283
## L_35_999_dev                          1.18335691537
## L_36_999_dev                          3.25213173780
## L_1_999_cal                           1.18058081966
## L_8_999_cal                          -0.00180679768
## L_10_999_cal                         -0.04581220530
## L_11_999_cal                         -0.00001361552
## L_13_999_cal                         -0.59595302430
## L_15_999_cal                          0.35077252912
## L_16_999_cal                          0.23459204631
## L_17_999_cal                          0.04789306573
## L_20_999_cal                          0.01820235408
## L_21_999_cal                          0.00556136411
## L_22_999_cal                         -0.05372169105
## L_24_999_cal                         -0.39892379289
## L_26_999_cal                          0.12054003466
## L_28_999_cal                          0.16486699374
## L_29_999_cal                          0.18948696365
## L_31_999_cal                          0.13696525004
## L_32_999_cal                         -0.83932204876
## L_33_999_cal                          0.45155052203
## L_34_999_cal                          0.03672725527
## L_35_999_cal                         -0.23243263792
## L_36_999_cal                          0.90085214853
## L_37_999_cal                         -0.54525536224
## L_38_999_cal                          0.50161020747
## L_39_999_cal                         -0.74904517614
## I_acc_ge_17*I_dev_ge_19               5.51496328897
## I_acc_ge_17*I_dev_ge_20               0.57649429567
## I_acc_ge_17*I_dev_ge_21             137.29149937434
## I_acc_ge_18*I_dev_ge_21               6.45465834886
## I_acc_ge_20*I_dev_ge_17              -1.11190434265
## I_acc_ge_21*I_dev_ge_9                0.20809357209
## I_dev_ge_10*I_cal_ge_31              -2.32990834511
## I_dev_ge_12*I_cal_ge_26              -0.80542294977
## I_dev_ge_13*I_cal_ge_37               1.16374345492
## I_dev_ge_14*I_cal_ge_26              -0.44214116825
## I_dev_ge_22*I_cal_ge_39               1.78463806790
## I_acc_ge_20*I_cal_ge_36              -0.18540685549
## I_acc_ge_20*I_cal_ge_37              -4.85927363521
## I_acc_ge_20*I_cal_ge_38              -2.86273908989
## I_acc_ge_21*I_cal_ge_31               0.29625560409&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the interactions at the end. Are these detecting the interaction that we know is in this simulated data set? We will find out below.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;It is also useful to look at actual and fitted plots for the different predictors. The paper shows a number of these.&lt;/p&gt;
&lt;p&gt;To produce these plots, we first add the column of fitted values to the data.frame holding the data using the &lt;code&gt;predict&lt;/code&gt; method for a &lt;code&gt;cv.glmnet&lt;/code&gt; object. Our selected model is that corresponding to the minimum CV error, which is corresponds to the &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; stored by the &lt;code&gt;cv_fit$lambda_min&lt;/code&gt; component of the &lt;code&gt;cv.glmnet&lt;/code&gt; object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;synthetic_data$fitted_lasso&amp;lt;- as.vector(exp(predict(cv_fit, newx = varset, s = cv_fit$lambda.min)) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function below produces the tracking graphs shown in the paper - plot values for all levels of one fundamental predictor holding a second predictor at a fixed value. To help with looking at the data, it also shades the past part of the data in grey.&lt;/p&gt;
&lt;p&gt;Since &lt;code&gt;ggplot2&lt;/code&gt; is a tidyverse package, it is easiest to put our data into &lt;em&gt;tidy&lt;/em&gt; format (essentially stacked/long output) prior to using &lt;code&gt;ggplot&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GraphModelVals&amp;lt;-function(data, primary_predictor, secondary_predictor, secondary_predictor_val, fitted_name, predictor_label){
    
    #extract data we want to use
    use_data &amp;lt;- data[data[[secondary_predictor]] == secondary_predictor_val,]

    
    # turn into tidy layout - note that we have simulated data, underlying mean, fitted data all to stack.
    data_tidy &amp;lt;- data.frame( 
      rep(use_data[[primary_predictor]], times=3),
      c(use_data[[&amp;quot;Y&amp;quot;]], use_data[[&amp;quot;mu&amp;quot;]], use_data[[fitted_name]]),
      c(rep(&amp;quot;Simulated&amp;quot;, times=nrow(use_data)), rep(&amp;quot;Underlying&amp;quot;, times=nrow(use_data)), rep(&amp;quot;Lasso&amp;quot;, times=nrow(use_data))) 
      )
    colnames(data_tidy) &amp;lt;- c(&amp;quot;predictor&amp;quot;, &amp;quot;values&amp;quot;, &amp;quot;data_labels&amp;quot;)
    
    data_tidy$values &amp;lt;- log(data_tidy$values)

    
    # extract values for past rectangle
    xmin1 &amp;lt;- min(use_data[[primary_predictor]][use_data$train_ind==TRUE])
    xmax1 &amp;lt;- max(use_data[[primary_predictor]][use_data$train_ind==TRUE])
    
    ymin1 &amp;lt;- min(data_tidy$values)*0.95
    ymax1 &amp;lt;- max(data_tidy$values)*1.05
    
    g &amp;lt;- ggplot(data=data_tidy, aes(x=predictor, y=values, group=data_labels))+
      geom_line(aes(linetype=data_labels, colour=data_labels, size=data_labels, alpha=data_labels))+
      scale_colour_manual(name=&amp;quot;&amp;quot;, values=c(&amp;quot;indianred4&amp;quot;, &amp;quot;slategrey&amp;quot;, &amp;quot;slategrey&amp;quot;))+
      scale_linetype_manual(name=&amp;quot;&amp;quot;, values=c(&amp;quot;solid&amp;quot;, &amp;quot;solid&amp;quot;, &amp;quot;dotted&amp;quot;))+
      scale_size_manual(name=&amp;quot;&amp;quot;, values=c(2,1,1))+
      scale_alpha_manual(name=&amp;quot;&amp;quot;, values=c(0.8, 0.5, 0.5))+
      theme_classic()+
      annotate(geom=&amp;quot;rect&amp;quot;, xmin=xmin1, xmax=xmax1, ymin=ymin1, ymax=ymax1, alpha=0.1)+
      theme(legend.position=&amp;quot;bottom&amp;quot;)+
      labs(x=predictor_label, y=&amp;quot;Log(Payments)&amp;quot;, title=paste(predictor_label, &amp;quot;tracking for&amp;quot;, secondary_predictor, &amp;quot;=&amp;quot;, secondary_predictor_val))
      
    

  invisible(list(data=data_tidy, graph=g))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s look at development quarter when accident quarter is 20. Remember the step-interaction starts at dev=21 - which we see in the graph.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dev_graph_list &amp;lt;- GraphModelVals(data = synthetic_data, 
                             primary_predictor = &amp;quot;dev&amp;quot;, 
                             secondary_predictor = &amp;quot;acc&amp;quot;, 
                             secondary_predictor_val = 20, 
                             fitted_name = &amp;quot;fitted_lasso&amp;quot;,
                             predictor_label = &amp;quot;Development quarter&amp;quot;)


dev_graph_list$graph&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-31-self-assembling-claim-reserving-models_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similarly we can look at accident quarter tracking when development quarter is 24 and again see the interaction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;acc_graph_list &amp;lt;- GraphModelVals(data = synthetic_data, 
                             primary_predictor = &amp;quot;acc&amp;quot;, 
                             secondary_predictor = &amp;quot;dev&amp;quot;, 
                             secondary_predictor_val = 24, 
                             fitted_name = &amp;quot;fitted_lasso&amp;quot;,
                             predictor_label = &amp;quot;Accident quarter&amp;quot;)


acc_graph_list$graph&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-31-self-assembling-claim-reserving-models_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You should, of course, carry out a full model validation exercise on any model prior to use, examining residuals, triangular heat maps and tweak the model if needed.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;claims-reserves&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Claims reserves&lt;/h1&gt;
&lt;p&gt;Finally, let’s get some claims reserve estimates and compare to those from an 8-period chain ladder.&lt;/p&gt;
&lt;p&gt;First let’s calculate the chainladder reserve.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# cumulative payments on training data set
synthetic_data_train$Ycum &amp;lt;- unlist(tapply(synthetic_data_train$Y, synthetic_data_train$acc, cumsum))

# calc cl factors using an 8 quarter average
cl_fac &amp;lt;- numeric(numperiods-1)

for (j in 1:numperiods-1){
  
  cl_fac[j] &amp;lt;- sum(subset(synthetic_data_train, dev == j+1 &amp;amp; acc &amp;gt; (numperiods-8-j) &amp;amp; acc &amp;lt;= numperiods-j)$Ycum) / 
    sum(subset(synthetic_data_train, dev == j &amp;amp; acc &amp;gt; (numperiods-8-j) &amp;amp; acc &amp;lt;= numperiods-j)$Ycum)
}


# accumulate the CL factors
cl_cum &amp;lt;- cumprod(rev(cl_fac))

# leading diagonal
leading_diagonal &amp;lt;- subset(synthetic_data_train, cal == numperiods &amp;amp; acc&amp;gt;1)$Ycum

# CL amounts now
cl_os &amp;lt;- cl_cum * leading_diagonal - leading_diagonal&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running the code shows that the reserve for accident period 40 is a bit high(!) - so the y-axis scale on any plots may need to be adjusted for display purposes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(&amp;quot;CL outstanding estimates&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;CL outstanding estimates&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head( data.frame(Accident=numperiods:2, os = rev(cl_os/1000000000)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Accident        os
## 1       40 488.32119
## 2       39  13.97485
## 3       38  21.60132
## 4       37  17.46894
## 5       36  21.27673
## 6       35  22.05057&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use the &lt;code&gt;tapply&lt;/code&gt; function to calculate the outstanding amounts by accident period for the LASSO model as well as the true values (both the “actual” simulated values and the true underlying values).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;synthetic_data_test &amp;lt;- subset(synthetic_data, train_ind == FALSE)

lasso_os &amp;lt;- tapply(synthetic_data_test$fitted_lasso, synthetic_data_test$acc, sum)
sim_os &amp;lt;- tapply(synthetic_data_test$Y, synthetic_data_test$acc, sum)
true_os &amp;lt;- tapply(synthetic_data_test$mu, synthetic_data_test$acc, sum)


#combine into a tidy dataset for ggplot
# add a linetype option to have different linetypes
compare_os &amp;lt;- data.frame(
  rep(2:numperiods, times=4), 
  c(sim_os, lasso_os, cl_os, true_os)/1000000000,
  c(rep(&amp;quot;Simulated&amp;quot;, times=numperiods-1), rep(&amp;quot;Lasso&amp;quot;, times=numperiods-1), rep(&amp;quot;Chainladder (8 qtr)&amp;quot;, times=numperiods-1), rep(&amp;quot;Underlying&amp;quot;, times=numperiods-1) )
)
colnames(compare_os)&amp;lt;-c(&amp;quot;Accident&amp;quot;, &amp;quot;Outstanding&amp;quot;, &amp;quot;data_labels&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a plot of the results (similar to that in the paper). Note that the y-axis is restricted for readability so that the actual chain ladder value for accident period 40 does not display on the graph.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;os_plot &amp;lt;-
    ggplot(data=compare_os, aes(x=Accident, y=Outstanding, group=data_labels))+
    geom_line(aes(colour=data_labels, linetype=data_labels), alpha=0.8, size=0.75)+
    scale_colour_manual(name=&amp;quot;&amp;quot;, values=c(&amp;quot;steelblue3&amp;quot;, &amp;quot;indianred4&amp;quot;, &amp;quot;slategrey&amp;quot;, &amp;quot;slategrey&amp;quot; ))+
    scale_linetype_manual(name=&amp;quot;&amp;quot;, values=c(&amp;quot;dashed&amp;quot;, &amp;quot;solid&amp;quot;, &amp;quot;dotted&amp;quot;, &amp;quot;solid&amp;quot; ))+
    geom_line(data=subset(compare_os, data_labels==&amp;quot;Lasso&amp;quot;), aes(colour=data_labels), size=1.25, alpha=0.8, colour=&amp;quot;indianred4&amp;quot;, linetype=&amp;quot;solid&amp;quot;)+
  coord_cartesian(ylim=c(0, 40))+
    theme_classic()+
    theme(legend.position=&amp;quot;bottom&amp;quot;, legend.title=element_blank())+
    scale_y_continuous(labels=scales::comma)+
    labs(x=&amp;quot;Accident quarter&amp;quot;, y=&amp;quot;Amount ($B)&amp;quot;)+
    NULL

os_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-31-self-assembling-claim-reserving-models_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can see from the graph that - despite the presence of an interaction affecting only a small number of cells, the LASSO model detects and responds appropriately to this change. By contrast, the chain ladder model does not perform so well.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;The aim of this article has been to demonstrate the fitting of a claims reserving model using a LASSO approach in R and to produce some of the model diagnostic and results output that a user might wish to examine. If you would like to experiment some more, you could try modifying the synthetic data set code to produce other types of simulated data (such as those in our paper), or try it out on a real data example.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-information&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Session information&lt;/h1&gt;
&lt;p&gt;To assist with reproducibility, here are details of my R session.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 3.6.1 (2019-07-05)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 18363)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_Australia.1252  LC_CTYPE=English_Australia.1252   
## [3] LC_MONETARY=English_Australia.1252 LC_NUMERIC=C                      
## [5] LC_TIME=English_Australia.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] ggplot2_3.2.1 glmnet_3.0-2  Matrix_1.2-17
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.3       compiler_3.6.1   pillar_1.4.2     iterators_1.0.12
##  [5] tools_3.6.1      digest_0.6.23    evaluate_0.14    lifecycle_0.1.0 
##  [9] tibble_2.1.3     gtable_0.3.0     lattice_0.20-38  pkgconfig_2.0.3 
## [13] rlang_0.4.2      foreach_1.4.7    yaml_2.2.0       blogdown_0.17   
## [17] xfun_0.11        withr_2.1.2      stringr_1.4.0    dplyr_0.8.3     
## [21] knitr_1.26       tidyselect_0.2.5 grid_3.6.1       glue_1.3.1      
## [25] R6_2.4.1         rmarkdown_2.0    bookdown_0.17    farver_2.0.1    
## [29] purrr_0.3.3      magrittr_1.5     scales_1.1.0     codetools_0.2-16
## [33] htmltools_0.4.0  assertthat_0.2.1 shape_1.4.4      colorspace_1.4-1
## [37] labeling_0.3     stringi_1.4.3    lazyeval_0.2.2   munsell_0.5.0   
## [41] crayon_1.3.4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Self-assembling insurance claim models using regularized regression and machine learning</title>
      <link>/publication/sagacious/</link>
      <pubDate>Fri, 31 Aug 2018 00:00:00 +0000</pubDate>
      <guid>/publication/sagacious/</guid>
      <description>


&lt;p&gt;The lasso is applied in an attempt to automate the loss reserving problem. The regression form contained within the lasso is a GLM, and so that the model has all the versatility of that type of model, but the model selection is automated and the parameter coefficients for selected terms will not be the same.&lt;/p&gt;
&lt;p&gt;There are two applications presented, one to synthetic data in conventional triangular form, and another to real data.The secret of success in such an endeavor is the selection of the set of candidate basis functions for representation of the data set. Cross-validation is used for model selection.&lt;/p&gt;
&lt;p&gt;The lasso performs well in modelling, identifying known features in the synthetic data, and tracking them accurately. This is despite complexity in those features that would challenge, and possibly defeat, most loss reserving alternatives. In the case of real data, the lasso also succeeds in tracking features of the data that analysis of the data set over many years has rendered virtually known.&lt;/p&gt;
&lt;p&gt;A later section of the paper discusses the prediction error associated with a lasso-based loss reserve. It is seen that the procedure can be readily adapted to the estimation of parameter and process error, but can also estimate one component of model error. To the authors knowledge, no other loss reserving model in the literature does so. &#34;&lt;/p&gt;
&lt;p&gt;See my &lt;a href=&#34;/post/self-assembling-claim-reserving-models&#34;&gt;blog post&lt;/a&gt; for a tutorial on how to use the method described in this paper.&lt;/p&gt;
&lt;!--

doi: &#34;&#34;


--&gt;
</description>
    </item>
    
    <item>
      <title>Stochastic loss reserving using Generalized Linear Models</title>
      <link>/publication/2016_monograph/</link>
      <pubDate>Wed, 04 May 2016 00:00:00 +0000</pubDate>
      <guid>/publication/2016_monograph/</guid>
      <description>


&lt;p&gt;The purpose of the monograph is to provide access to generalized linear models for loss reserving but initially with strong emphasis on the chain ladder. The chain ladder is formulated in a GLM context, as is the statistical distribution of the loss reserve. This structure is then used to test the need for departure from the chain ladder model and to formulate any required model extensions.&lt;/p&gt;
&lt;p&gt;The chain ladder is by far the most widely used method for loss reserving. The chain ladder algorithm itself is non-stochastic, but Mack (1993) defined a stochastic version of the model and showed how a mean square error of prediction may be associated with any loss reserve obtained from this model.&lt;/p&gt;
&lt;p&gt;There are, however, two families of stochastic model which generate the chain ladder algorithm for the estimation of loss reserve, as discussed in Taylor (2011). They require differing treatments of for the estimation of mean square error of prediction. Both families of model may be formulated as generalized linear models. This is not widely appreciated of the Mack model. The monograph commences with the identification of these two families and their respective GLM formulations.&lt;/p&gt;
&lt;p&gt;GLM formulation naturally invites the use of a bootstrap to estimate prediction error. The bootstrap estimates the entire distribution of loss reserve rather than just the mean square error of prediction obtainable from Mack’s algorithm. The monograph discusses both parametric and semi-parametric forms of the GLM bootstrap.&lt;/p&gt;
&lt;p&gt;Emphasis is placed on the use of statistical software to implement the GLM formulation. This formulation and the associated software provide the diagnostics for testing the validity of the model. This aspect is covered by the existing literature but the monograph reviews this material in view of its importance.&lt;/p&gt;
&lt;p&gt;Practical applications of the chain ladder often depart from the strict model. There are a number of causes but prominent among these are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the need to smooth the age-to-age factor tail;&lt;/li&gt;
&lt;li&gt;the need to give greater weight to more recent data than to older.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These two matters are considered within the GLM context. The subject of smoothing leads to a discussion of generalized additive models.&lt;/p&gt;
&lt;p&gt;As regards the second point, the GLM structure is used to test whether or not data are time-homogeneous (as is required by the strict chain ladder model) and, if not, to suggest a procedure for recognising and accommodating time-heterogeneity in the data. This may lead to the common practice of discarding all but the last m diagonals of the claim triangle, but more general approaches are also be considered.&lt;/p&gt;
&lt;p&gt;As time-heterogeneity is not consistent with the chain ladder model, it amounts to model failure, and is recognizable from the diagnostics introduced above. Various forms of model failure are considered and, in each case, a model extension constructed to deal with it.&lt;/p&gt;
&lt;p&gt;Finally, extension to several models that go beyond the scope of generalized linear models is discussed.&lt;/p&gt;
&lt;p&gt;See my &lt;a href=&#34;/post/traditional-style-reserving-using-glms&#34;&gt;blog post&lt;/a&gt; for some R code on how to fit the GLM described in the monograph.&lt;/p&gt;
&lt;!--

---
abstract: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus
  ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed
  ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis
  sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida
  egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id
  dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.
  Vestibulum sit amet erat at nulla eleifend gravida.
authors:
- admin
- Robert Ford
date: &#34;2015-09-01T00:00:00Z&#34;
doi: &#34;&#34;
featured: false
image:
  caption: &#39;Image credit: [**Unsplash**](https://unsplash.com/photos/jdD8gXaTZsc)&#39;
  focal_point: &#34;&#34;
  preview_only: false
projects: []
publication: &#39;*Journal of Source Themes, 1*(1)&#39;
publication_short: &#34;&#34;
publication_types:
- &#34;2&#34;
publishDate: &#34;2017-01-01T00:00:00Z&#34;
slides: example
summary: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus
  ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.
tags:
- Source Themes
title: An example journal article
url_code: &#34;&#34;
url_dataset: &#34;&#34;
url_pdf: http://arxiv.org/pdf/1512.04133v1
url_poster: &#34;&#34;
url_project: &#34;&#34;
url_slides: &#34;&#34;
url_source: &#34;&#34;
url_video: &#34;&#34;
---

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/).


--&gt;
</description>
    </item>
    
    <item>
      <title>Individual claim loss reserving conditioned by case estimates</title>
      <link>/publication/2008_bootstrap/</link>
      <pubDate>Mon, 01 Sep 2008 00:00:00 +0000</pubDate>
      <guid>/publication/2008_bootstrap/</guid>
      <description>


&lt;p&gt;This paper examines various forms of individual claim model for the purpose of loss reserving, with emphasis on the prediction error associated with the reserve. Each form of model is calibrated against a single extensive data set, and then used to generate a forecast of loss reserve and an estimate of its prediction error.&lt;/p&gt;
&lt;p&gt;The basis of this is a model of the “paids” type, in which the sizes of strictly positive individual finalised claims are expressed in terms of a small number of covariates, most of which are in some way functions of time. Such models can be found in the literature.&lt;/p&gt;
&lt;p&gt;The purpose of the current paper is to extend these to individual claim “incurreds” models. These are constructed by the inclusion of case estimates in the model’s conditioning information. This form of model is found to involve rather more complexity in its structure.&lt;/p&gt;
&lt;p&gt;For the particular data set considered here, this did not yield any direct improvement in prediction error. However, a blending of the paids and incurreds models did so.&lt;/p&gt;
&lt;!--

doi: &#34;&#34;


--&gt;
</description>
    </item>
    
    <item>
      <title>Loss reserving with GLMs: a case study</title>
      <link>/publication/2004_cas_glms/</link>
      <pubDate>Sun, 16 May 2004 00:00:00 +0000</pubDate>
      <guid>/publication/2004_cas_glms/</guid>
      <description>


&lt;p&gt;This paper provides a case study in the application of generalised linear models (GLMs) to loss reserving. The study is motivated by approaching the exercise from the viewpoint of an actuary with a predisposition to the application of the chain ladder (CL). The data set under study is seen to violate the conditions for application of the CL in a number of ways. The difficulties of adjusting the CL to allow for these features are noted (Section 3).&lt;/p&gt;
&lt;p&gt;Regression, and particularly GLM regression, is introduced as a structured and rigorous form of dat analysis. This enables the investigation and modelling of a number of complex features of the data responsible for the violation of the CL conditions. These include superimposed inflation and changes in the rules governing the payment of claims (Sections 4 to 7). The development of the analysis is traced in some detail, as is the production of a range of diagnostics and tests used to compare candidate models and validate the final one.&lt;/p&gt;
&lt;p&gt;The benefits of this approach are dicussed in Section 8&#34;&lt;/p&gt;
&lt;!--

doi: &#34;&#34;


--&gt;
</description>
    </item>
    
  </channel>
</rss>
